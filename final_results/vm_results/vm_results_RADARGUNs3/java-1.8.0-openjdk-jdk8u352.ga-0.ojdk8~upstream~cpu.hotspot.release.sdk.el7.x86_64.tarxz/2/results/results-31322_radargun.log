12:40:39,263 INFO  [org.radargun.RemoteMasterConnection] (main) Attempting to connect to master results:2103
12:40:39,270 INFO  [org.radargun.RemoteMasterConnection] (main) Successfully established connection with master at: results:2103
12:40:39,271 INFO  [org.radargun.Slave] (main) Received slave index 0
12:40:39,271 INFO  [org.radargun.Slave] (main) Received slave count 3
12:40:39,449 INFO  [org.radargun.ServiceHelper] (sc-main) ServiceContext properties: {}
12:40:39,548 INFO  [org.radargun.Slave] (sc-main) Eager Service Infinispan92EmbeddedService {batching=false, cache=testCache, channelRetrievalTimeout=2 mins 0 secs, enableDiagnostics=null, explicitLocking=false, file=/mnt/workspace/results/dist-sync.xml, internalsExpositionEnabled=false, jgroupsDumperEnabled=false, jgroupsDumperInterval=10.000 secs, keysPerThread=-1, mapReduceDistributedReducePhase=false, mapReduceUseIntermediateSharedCache=false, removedCaches=[  ], threadsPerNode=-1 } loaded.
12:40:41,260 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioInit
12:40:41,515 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioInit
12:40:41,519 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:40:41,527 INFO  [org.radargun.Slave] (sc-main) Starting stage BeforeServiceStart
12:40:41,527 INFO  [org.radargun.Slave] (sc-main) Finished stage BeforeServiceStart
12:40:41,527 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:40:41,536 INFO  [org.radargun.Slave] (sc-main) Starting stage ServiceStart
12:40:41,536 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Startup staggering, this is the slave with index 0, not sleeping
12:40:41,537 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Ack master's StartCluster stage. Local address is: /127.0.0.1. This slave's index is: 0
12:40:41,539 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) Infinispan version: Infinispan 'Gaina' 9.2.0.Final
12:40:41,550 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) JGroups version: JGroups 4.0.10.Final (Schiener Berg)
12:40:42,000 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000078: Starting JGroups channel results
12:40:42,075 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
12:40:42,075 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 20.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
12:40:42,075 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
12:40:42,076 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 25.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
12:40:47,117 INFO  [org.infinispan.CLUSTER] (sc-main) ISPN000094: Received new cluster view for channel results: [fedora-26449|0] (1) [fedora-26449]
12:40:47,223 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000079: Channel results local address is fedora-26449, physical addresses are [192.168.121.156:45628]
12:40:47,226 INFO  [org.infinispan.factories.GlobalComponentRegistry] (sc-main) ISPN000128: Infinispan version: Infinispan 'Gaina' 9.2.0.Final
12:40:47,813 INFO  [org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup] (sc-main) ISPN000107: Retrieving transaction manager Transaction: unknown
12:40:47,907 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-26449) ISPN000094: Received new cluster view for channel results: [fedora-26449|1] (2) [fedora-26449, fedora-24786]
12:40:47,914 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-26449) ISPN100000: Node fedora-24786 joined the cluster
12:40:47,928 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
12:40:47,928 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
12:40:47,931 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) ([fedora-26449(local=true, coord=true), fedora-24786(local=false, coord=false)]) Number of members=2 is not the one expected: 3
12:40:48,430 INFO  [org.infinispan.CLUSTER] (jgroups-10,fedora-26449) ISPN000094: Received new cluster view for channel results: [fedora-26449|2] (3) [fedora-26449, fedora-24786, fedora-18326]
12:40:48,432 INFO  [org.infinispan.CLUSTER] (jgroups-10,fedora-26449) ISPN100000: Node fedora-18326 joined the cluster
12:40:48,523 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-26449: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-26449: 129, fedora-24786: 127]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:40:48,525 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 2
12:40:48,524 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-26449: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-26449: 129, fedora-24786: 127]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:40:48,525 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 2
12:40:48,571 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 2
12:40:48,578 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=___counter_configuration][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 2
12:40:48,706 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 2
12:40:48,706 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 2
12:40:48,711 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 2
12:40:48,712 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 2
12:40:48,712 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t3) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 3
12:40:48,713 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t5) [Context=___counter_configuration][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 3
12:40:48,717 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 3
12:40:48,719 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t8) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 4
12:40:48,719 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 3
12:40:48,721 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=___counter_configuration][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 4
12:40:48,732 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 4
12:40:48,734 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 4
12:40:48,806 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=256, owners = (1)[fedora-26449: 256+0]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-26449: 129+127, fedora-24786: 127+129]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:40:48,806 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 2
12:40:48,814 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=___counters][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 2
12:40:48,894 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 2
12:40:48,897 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 2
12:40:48,899 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=___counters][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 3
12:40:48,903 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 3
12:40:48,905 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=___counters][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 4
12:40:48,907 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 4
12:40:48,931 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Number of members is the one expected: 3
12:40:48,931 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Successfully started cache service infinispan92/embedded on slave 0
12:40:48,995 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheAvailability, cacheName, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[?:1.8.0_352-internal]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:1.8.0_352-internal]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
12:40:49,005 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheAvailability, cacheName, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[?:1.8.0_352-internal]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:1.8.0_352-internal]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
12:40:49,029 INFO  [org.radargun.Slave] (sc-main) Finished stage ServiceStart
12:40:49,040 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:40:49,087 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-26449: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-26449: 129, fedora-24786: 127]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:40:49,088 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 2
12:40:49,094 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 2
12:40:49,130 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=512, owners = (1)[fedora-26449: 512+0]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-26449: 253+259, fedora-24786: 259+253]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:40:49,130 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 2
12:40:49,131 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 2
12:40:49,134 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 2
12:40:49,136 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 3
12:40:49,142 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 3
12:40:49,143 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=testCache][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 2
12:40:49,145 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 4
12:40:49,153 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 4
12:40:49,171 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 2
12:40:49,171 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 2
12:40:49,174 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=testCache][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 3
12:40:49,178 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 3
12:40:49,180 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=testCache][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 4
12:40:49,182 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 4
12:40:49,268 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-26449: 129, fedora-24786: 127]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-26449: 90, fedora-24786: 79, fedora-18326: 87]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786, fedora-18326], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef, 71b77a3a-7077-4cf3-8d66-959359f9ca70]}
12:40:49,268 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 6
12:40:49,270 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-26449: 129, fedora-24786: 127]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-26449: 90, fedora-24786: 79, fedora-18326: 87]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786, fedora-18326], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef, 71b77a3a-7077-4cf3-8d66-959359f9ca70]}
12:40:49,271 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 6
12:40:49,271 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 6
12:40:49,271 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=___counter_configuration][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 6
12:40:49,274 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 6
12:40:49,274 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 6
12:40:49,337 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 6
12:40:49,338 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 6
12:40:49,339 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___counter_configuration][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 7
12:40:49,341 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 7
12:40:49,345 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 6
12:40:49,345 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 6
12:40:49,346 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 7
12:40:49,348 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 7
12:40:49,348 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 7
12:40:49,349 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 7
12:40:49,349 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t4) [Context=___counter_configuration][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 8
12:40:49,351 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 8
12:40:49,352 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 8
12:40:49,350 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=org.infinispan.CONFIG][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 8
12:40:49,352 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 8
12:40:49,355 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 8
12:40:49,389 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-26449: 129+127, fedora-24786: 127+129]}, pendingCH=DefaultConsistentHash{ns=256, owners = (3)[fedora-26449: 90+85, fedora-24786: 79+84, fedora-18326: 87+87]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786, fedora-18326], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef, 71b77a3a-7077-4cf3-8d66-959359f9ca70]}
12:40:49,391 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 6
12:40:49,394 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t18) [Context=___counters][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 6
12:40:49,395 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 6
12:40:49,417 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 6
12:40:49,417 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 6
12:40:49,418 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=___counters][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 7
12:40:49,423 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 7
12:40:49,423 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 7
12:40:49,425 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=___counters][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 8
12:40:49,429 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 8
12:40:49,427 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 8
12:40:49,525 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-26449: 129, fedora-24786: 127]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-26449: 90, fedora-24786: 79, fedora-18326: 87]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786, fedora-18326], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef, 71b77a3a-7077-4cf3-8d66-959359f9ca70]}
12:40:49,525 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 6
12:40:49,527 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t3) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 6
12:40:49,531 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 6
12:40:49,545 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 6
12:40:49,546 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 6
12:40:49,547 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 7
12:40:49,548 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 7
12:40:49,548 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 7
12:40:49,550 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=___protobuf_metadata][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 8
12:40:49,552 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 8
12:40:49,554 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 8
12:40:49,567 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-26449: 253+259, fedora-24786: 259+253]}, pendingCH=DefaultConsistentHash{ns=512, owners = (3)[fedora-26449: 172+183, fedora-24786: 160+166, fedora-18326: 180+163]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786, fedora-18326], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef, 71b77a3a-7077-4cf3-8d66-959359f9ca70]}
12:40:49,567 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 6
12:40:49,569 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t14) [Context=testCache][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 6
12:40:49,573 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 6
12:40:49,587 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 6
12:40:49,587 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 6
12:40:49,591 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t15) [Context=testCache][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 7
12:40:49,593 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 7
12:40:49,595 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 7
12:40:49,596 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=testCache][Scope=fedora-26449]ISPN100003: Node fedora-26449 finished rebalance phase with topology id 8
12:40:49,598 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-18326]ISPN100003: Node fedora-18326 finished rebalance phase with topology id 8
12:40:49,599 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 8
12:40:49,685 INFO  [org.radargun.Slave] (sc-main) Starting stage AfterServiceStart
12:40:49,686 INFO  [org.radargun.Slave] (sc-main) Finished stage AfterServiceStart
12:40:49,687 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:40:49,754 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStart
12:40:49,763 INFO  [org.radargun.sysmonitor.AbstractMonitors] (sc-main) Gathering statistics every 1000 ms
12:40:49,764 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStart
12:40:49,764 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:40:49,790 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
12:40:54,988 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) This node loaded 10000 entries (~10000000 bytes)
12:40:56,810 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) This node loaded 20000 entries (~20000000 bytes)
12:40:58,413 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 30000 entries (~30000000 bytes)
12:40:58,817 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
12:40:58,836 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
12:40:58,838 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
12:40:58,852 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
12:40:58,854 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
12:40:58,854 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
12:40:58,878 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
12:40:58,879 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
12:40:58,883 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
12:40:58,884 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
12:40:58,884 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
12:40:58,885 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:40:58,908 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
12:40:58,911 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
12:40:58,911 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
12:40:58,911 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
12:40:58,911 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
12:40:58,912 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test warmup
12:40:58,915 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
12:41:58,922 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 1 mins 0 secs
12:41:58,924 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
12:41:58,936 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:41:58,964 INFO  [org.radargun.Slave] (sc-main) Starting stage Clear
12:41:58,966 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) Before executing clear, memory looks like this: 
Runtime free: 1,059,079 kb
Runtime max:1,310,208 kb
Runtime total:1,310,208 kb
MX Code Cache(Non-heap memory): used: 13,248 kb, init: 2,496 kb, committed: 14,208 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 37,967 kb, init: 0 kb, committed: 40,064 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,605 kb, init: 0 kb, committed: 4,992 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 106,407 kb, init: 350,208 kb, committed: 290,304 kb, max: 293,376 kb
MX PS Survivor Space(Heap memory): used: 70,784 kb, init: 57,856 kb, committed: 87,040 kb, max: 87,040 kb
MX PS Old Gen(Heap memory): used: 73,936 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
12:41:59,311 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) After executing clear, memory looks like this: 
Runtime free: 1,300,389 kb
Runtime max:1,314,816 kb
Runtime total:1,314,816 kb
MX Code Cache(Non-heap memory): used: 13,436 kb, init: 2,496 kb, committed: 14,336 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 37,841 kb, init: 0 kb, committed: 40,064 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,555 kb, init: 0 kb, committed: 4,992 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 1,709 kb, init: 350,208 kb, committed: 296,448 kb, max: 296,448 kb
MX PS Survivor Space(Heap memory): used: 0 kb, init: 57,856 kb, committed: 85,504 kb, max: 85,504 kb
MX PS Old Gen(Heap memory): used: 12,717 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
12:41:59,311 INFO  [org.radargun.Slave] (sc-main) Finished stage Clear
12:41:59,312 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:41:59,321 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
12:42:00,772 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) This node loaded 10000 entries (~10000000 bytes)
12:42:02,195 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) This node loaded 20000 entries (~20000000 bytes)
12:42:03,645 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 30000 entries (~30000000 bytes)
12:42:04,172 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
12:42:04,183 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
12:42:04,188 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
12:42:04,189 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
12:42:04,201 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
12:42:04,217 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
12:42:04,227 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
12:42:04,230 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
12:42:04,230 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
12:42:04,237 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
12:42:04,238 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
12:42:04,238 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:42:04,634 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
12:42:04,635 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
12:42:04,635 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
12:42:04,635 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
12:42:04,636 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
12:42:04,636 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test stress-test
12:42:04,648 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
12:52:04,650 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 10 mins 0 secs
12:52:04,651 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
12:52:04,766 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:52:04,873 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStop
12:52:04,874 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStop
12:52:04,874 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:52:04,888 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioDestroy
12:52:04,888 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Scenario finished, destroying...
12:52:04,888 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Memory before cleanup: 
Runtime free: 966,835 kb
Runtime max:1,325,568 kb
Runtime total:1,325,568 kb
MX Code Cache(Non-heap memory): used: 14,925 kb, init: 2,496 kb, committed: 15,104 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 38,403 kb, init: 0 kb, committed: 40,704 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,590 kb, init: 0 kb, committed: 5,120 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 234,514 kb, init: 350,208 kb, committed: 319,488 kb, max: 319,488 kb
MX PS Survivor Space(Heap memory): used: 71,968 kb, init: 57,856 kb, committed: 73,216 kb, max: 73,216 kb
MX PS Old Gen(Heap memory): used: 52,249 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
12:52:04,889 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Stopping service.
12:52:04,916 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t37) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-26449: 264+91, fedora-24786: 248+78]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-26449: 253+259, fedora-24786: 259+253]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:52:04,921 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t37) [Context=testCache][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 11
12:52:04,933 WARN  [org.infinispan.CLUSTER] (StopThread) [Context=testCache]ISPN000312: Lost data because of graceful leaver fedora-26449
12:52:04,962 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t29) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-26449: 136+39, fedora-24786: 120+43]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-26449: 129+127, fedora-24786: 127+129]}, unionCH=null, actualMembers=[fedora-26449, fedora-24786], persistentUUIDs=[55bc6d16-9516-41f6-8960-48e7f2288055, b89959f1-be46-487a-8a0d-933e44a537ef]}
12:52:04,963 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t29) [Context=___counters][Scope=fedora-26449]ISPN100002: Started rebalance with topology id 11
12:52:05,021 WARN  [org.infinispan.CLUSTER] (StopThread) [Context=___counters]ISPN000312: Lost data because of graceful leaver fedora-26449
12:52:05,024 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t36) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 11
12:52:05,026 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t38) [Context=___counters][Scope=fedora-24786]ISPN100003: Node fedora-24786 finished rebalance phase with topology id 12
12:52:05,030 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t38) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counters, type=REBALANCE_PHASE_CONFIRM, sender=fedora-24786, joinInfo=null, topologyId=12, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-24786 for cache ___counters, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352-internal]
12:52:05,030 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t36) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counters, type=REBALANCE_PHASE_CONFIRM, sender=fedora-24786, joinInfo=null, topologyId=11, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-24786 for cache ___counters, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352-internal]
12:52:05,034 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t2) ISPN000210: Failed to request state of cache testCache from node fedora-24786, segments {3 9-12 17-19 25 28-34 40-41 46 52-53 65-66 70-71 83-84 111-114 117-119 125-127 134-136 141-149 152 156-157 167-173 181 198 202-203 210 217-228 238-248 252 259-263 276-277 280-281 294-295 312-316 325 332-335 347-353 366 395 399-400 404 416 420-423 426-433 438 452-456 471-473 476-478 486-491 494-495 501 505-506}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-24786 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352-internal]
12:52:05,041 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (StopThread) ISPN000080: Disconnecting JGroups channel results
12:52:05,043 INFO  [org.infinispan.CLUSTER] (jgroups-42,fedora-26449) ISPN000094: Received new cluster view for channel results: [fedora-26449|3] (2) [fedora-26449, fedora-24786]
12:52:05,044 INFO  [org.infinispan.CLUSTER] (jgroups-42,fedora-26449) ISPN100001: Node fedora-18326 left the cluster
12:52:05,078 INFO  [org.radargun.service.Infinispan90Lifecycle] (StopThread) Stopped, previous view is [fedora-26449, fedora-24786, fedora-18326]
12:52:05,078 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Service successfully stopped.
12:52:05,078 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioDestroy
12:52:05,079 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
12:52:06,132 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.service.Infinispan80EmbeddedService.destroy() overrides public void org.radargun.service.Infinispan60EmbeddedService.destroy() but both are declared with @Destroy annotation: calling only once
12:52:06,135 INFO  [org.radargun.Slave] (main) Starting stage ScenarioCleanup
12:52:06,299 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Memory after cleanup: 
Runtime free: 1,307,346 kb
Runtime max:1,325,568 kb
Runtime total:1,325,568 kb
MX Code Cache(Non-heap memory): used: 15,129 kb, init: 2,496 kb, committed: 15,296 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 38,531 kb, init: 0 kb, committed: 40,704 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,614 kb, init: 0 kb, committed: 5,120 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 5,587 kb, init: 350,208 kb, committed: 319,488 kb, max: 319,488 kb
MX PS Survivor Space(Heap memory): used: 0 kb, init: 57,856 kb, committed: 73,216 kb, max: 73,216 kb
MX PS Old Gen(Heap memory): used: 12,633 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
12:52:06,301 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
12:52:11,406 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
12:52:15,540 INFO  [org.radargun.Slave] (main) Master shutdown!
12:52:15,541 INFO  [org.radargun.ShutDownHook] (Thread-0) Slave process is being shutdown
