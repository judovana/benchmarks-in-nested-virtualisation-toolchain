23:20:46,372 INFO  [org.radargun.RemoteMasterConnection] (main) Attempting to connect to master results:2103
23:20:46,380 INFO  [org.radargun.RemoteMasterConnection] (main) Successfully established connection with master at: results:2103
23:20:46,382 INFO  [org.radargun.Slave] (main) Received slave index 0
23:20:46,382 INFO  [org.radargun.Slave] (main) Received slave count 3
23:20:46,694 INFO  [org.radargun.ServiceHelper] (sc-main) ServiceContext properties: {}
23:20:46,820 INFO  [org.radargun.Slave] (sc-main) Eager Service Infinispan92EmbeddedService {batching=false, cache=testCache, channelRetrievalTimeout=2 mins 0 secs, enableDiagnostics=null, explicitLocking=false, file=/mnt/workspace/results/dist-sync.xml, internalsExpositionEnabled=false, jgroupsDumperEnabled=false, jgroupsDumperInterval=10.000 secs, keysPerThread=-1, mapReduceDistributedReducePhase=false, mapReduceUseIntermediateSharedCache=false, removedCaches=[  ], threadsPerNode=-1 } loaded.
23:20:48,659 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioInit
23:20:49,062 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioInit
23:20:49,065 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:20:49,073 INFO  [org.radargun.Slave] (sc-main) Starting stage BeforeServiceStart
23:20:49,074 INFO  [org.radargun.Slave] (sc-main) Finished stage BeforeServiceStart
23:20:49,075 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:20:49,088 INFO  [org.radargun.Slave] (sc-main) Starting stage ServiceStart
23:20:49,090 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Startup staggering, this is the slave with index 0, not sleeping
23:20:49,093 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Ack master's StartCluster stage. Local address is: /127.0.0.1. This slave's index is: 0
23:20:49,096 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) Infinispan version: Infinispan 'Gaina' 9.2.0.Final
23:20:49,114 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) JGroups version: JGroups 4.0.10.Final (Schiener Berg)
23:20:49,698 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000078: Starting JGroups channel results
23:20:49,785 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
23:20:49,785 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 20.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
23:20:49,786 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
23:20:49,786 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 25.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
23:20:54,845 INFO  [org.infinispan.CLUSTER] (sc-main) ISPN000094: Received new cluster view for channel results: [fedora-42583|0] (1) [fedora-42583]
23:20:54,959 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000079: Channel results local address is fedora-42583, physical addresses are [192.168.124.175:40958]
23:20:54,962 INFO  [org.infinispan.factories.GlobalComponentRegistry] (sc-main) ISPN000128: Infinispan version: Infinispan 'Gaina' 9.2.0.Final
23:20:55,640 INFO  [org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup] (sc-main) ISPN000107: Retrieving transaction manager Transaction: unknown
23:20:55,733 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
23:20:55,733 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
23:20:55,735 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) ([fedora-42583(local=true, coord=true)]) Number of members=1 is not the one expected: 3
23:20:56,005 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-42583) ISPN000094: Received new cluster view for channel results: [fedora-42583|1] (2) [fedora-42583, fedora-12606]
23:20:56,019 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-42583) ISPN100000: Node fedora-12606 joined the cluster
23:20:56,303 INFO  [org.infinispan.CLUSTER] (jgroups-12,fedora-42583) ISPN000094: Received new cluster view for channel results: [fedora-42583|2] (3) [fedora-42583, fedora-12606, fedora-14249]
23:20:56,309 INFO  [org.infinispan.CLUSTER] (jgroups-12,fedora-42583) ISPN100000: Node fedora-14249 joined the cluster
23:20:56,735 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Number of members is the one expected: 3
23:20:56,735 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Successfully started cache service infinispan92/embedded on slave 0
23:20:56,850 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-42583: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:20:56,852 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 2
23:20:56,857 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-42583: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:20:56,863 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 2
23:20:56,874 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[?:1.8.0_342-internal]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:1.8.0_342-internal]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
23:20:56,912 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[?:1.8.0_342-internal]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:1.8.0_342-internal]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
23:20:56,929 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 2
23:20:56,930 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=___counter_configuration][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 2
23:20:56,978 INFO  [org.radargun.Slave] (sc-main) Finished stage ServiceStart
23:20:57,018 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:20:57,103 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 2
23:20:57,104 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 2
23:20:57,107 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t3) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 3
23:20:57,112 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 2
23:20:57,114 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 2
23:20:57,115 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t4) [Context=___counter_configuration][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 3
23:20:57,119 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 3
23:20:57,124 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t9) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 4
23:20:57,128 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 3
23:20:57,130 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 3
23:20:57,133 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t2) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=org.infinispan.CONFIG, type=REBALANCE_PHASE_CONFIRM, sender=fedora-14249, joinInfo=null, topologyId=3, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-14249 for cache org.infinispan.CONFIG, expecting topology id 4 but got 3
	at org.infinispan.topology.RebalanceConfirmationCollector.confirmPhase(RebalanceConfirmationCollector.java:41) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:337) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:20:57,133 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 3
23:20:57,140 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 4
23:20:57,143 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-42583: 84, fedora-12606: 90, fedora-14249: 82]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606, fedora-14249], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa, b39f6cde-bcd8-4c56-bdac-448952eba963]}
23:20:57,147 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 6
23:20:57,142 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t12) [Context=___counter_configuration][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 4
23:20:57,151 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t17) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 6
23:20:57,153 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=org.infinispan.CONFIG][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 4
23:20:57,155 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t3) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=org.infinispan.CONFIG, type=REBALANCE_PHASE_CONFIRM, sender=fedora-14249, joinInfo=null, topologyId=4, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-14249 for cache org.infinispan.CONFIG, expecting topology id 6 but got 4
	at org.infinispan.topology.RebalanceConfirmationCollector.confirmPhase(RebalanceConfirmationCollector.java:41) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:337) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:20:57,163 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 4
23:20:57,195 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 4
23:20:57,200 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 6
23:20:57,202 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-42583: 84, fedora-12606: 90, fedora-14249: 82]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606, fedora-14249], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa, b39f6cde-bcd8-4c56-bdac-448952eba963]}
23:20:57,203 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 6
23:20:57,207 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=___counter_configuration][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 6
23:20:57,247 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 6
23:20:57,281 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=256, owners = (1)[fedora-42583: 256+0]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-42583: 131+125, fedora-12606: 125+131]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:20:57,283 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counters][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 2
23:20:57,286 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 2
23:20:57,299 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-42583: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:20:57,300 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 2
23:20:57,310 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 2
23:20:57,342 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=512, owners = (1)[fedora-42583: 512+0]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-42583: 256+256, fedora-12606: 256+256]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:20:57,343 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 2
23:20:57,355 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 2
23:20:57,387 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 2
23:20:57,387 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 2
23:20:57,389 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t13) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 3
23:20:57,390 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 2
23:20:57,391 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 2
23:20:57,393 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t12) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 3
23:20:57,414 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 6
23:20:57,415 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 6
23:20:57,415 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 3
23:20:57,417 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___counter_configuration][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 7
23:20:57,415 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counters][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 3
23:20:57,420 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 4
23:20:57,421 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 4
23:20:57,429 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=testCache][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 2
23:20:57,428 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t4) [Context=___counter_configuration][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 7
23:20:57,439 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 4
23:20:57,435 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 4
23:20:57,437 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 2
23:20:57,447 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-42583: 84, fedora-12606: 90, fedora-14249: 82]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606, fedora-14249], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa, b39f6cde-bcd8-4c56-bdac-448952eba963]}
23:20:57,448 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 6
23:20:57,448 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 3
23:20:57,453 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 6
23:20:57,455 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 3
23:20:57,457 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t12) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 4
23:20:57,466 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 6
23:20:57,466 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 6
23:20:57,467 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t9) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 7
23:20:57,470 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 4
23:20:57,470 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=org.infinispan.CONFIG][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 7
23:20:57,470 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 6
23:20:57,485 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 4
23:20:57,486 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t2) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___protobuf_metadata, type=REBALANCE_PHASE_CONFIRM, sender=fedora-14249, joinInfo=null, topologyId=4, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-14249 for cache ___protobuf_metadata, expecting topology id 6 but got 4
	at org.infinispan.topology.RebalanceConfirmationCollector.confirmPhase(RebalanceConfirmationCollector.java:41) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:337) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:20:57,486 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 3
23:20:57,488 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t1) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___protobuf_metadata, type=REBALANCE_PHASE_CONFIRM, sender=fedora-14249, joinInfo=null, topologyId=3, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-14249 for cache ___protobuf_metadata, expecting topology id 6 but got 3
	at org.infinispan.topology.RebalanceConfirmationCollector.confirmPhase(RebalanceConfirmationCollector.java:41) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:337) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:20:57,499 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 7
23:20:57,501 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___counter_configuration][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 8
23:20:57,503 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 8
23:20:57,508 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 7
23:20:57,510 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 8
23:20:57,516 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 8
23:20:57,522 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-42583: 256+256, fedora-12606: 256+256]}, pendingCH=DefaultConsistentHash{ns=512, owners = (3)[fedora-42583: 173+172, fedora-12606: 173+173, fedora-14249: 166+167]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606, fedora-14249], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa, b39f6cde-bcd8-4c56-bdac-448952eba963]}
23:20:57,523 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 6
23:20:57,530 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 6
23:20:57,532 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 8
23:20:57,547 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 6
23:20:57,570 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 8
23:20:57,585 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-42583: 131+125, fedora-12606: 125+131]}, pendingCH=DefaultConsistentHash{ns=256, owners = (3)[fedora-42583: 84+89, fedora-12606: 90+90, fedora-14249: 82+77]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606, fedora-14249], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa, b39f6cde-bcd8-4c56-bdac-448952eba963]}
23:20:57,590 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 6
23:20:57,593 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t14) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 6
23:20:57,596 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 6
23:20:57,631 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 6
23:20:57,632 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 6
23:20:57,637 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 7
23:20:57,641 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 7
23:20:57,643 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 6
23:20:57,644 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 6
23:20:57,645 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 7
23:20:57,647 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 7
23:20:57,648 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 6
23:20:57,648 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 6
23:20:57,649 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 7
23:20:57,651 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 8
23:20:57,652 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 7
23:20:57,656 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 8
23:20:57,656 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 8
23:20:57,658 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 7
23:20:57,660 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=testCache][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 7
23:20:57,664 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 7
23:20:57,667 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t8) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 8
23:20:57,669 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 8
23:20:57,671 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t14) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 8
23:20:57,672 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=testCache][Scope=fedora-14249]ISPN100003: Node fedora-14249 finished rebalance phase with topology id 8
23:20:57,682 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 8
23:20:57,682 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___protobuf_metadata][Scope=fedora-12606]ISPN100003: Node fedora-12606 finished rebalance phase with topology id 8
23:20:57,813 INFO  [org.radargun.Slave] (sc-main) Starting stage AfterServiceStart
23:20:57,814 INFO  [org.radargun.Slave] (sc-main) Finished stage AfterServiceStart
23:20:57,818 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:20:57,892 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStart
23:20:57,899 INFO  [org.radargun.sysmonitor.AbstractMonitors] (sc-main) Gathering statistics every 1000 ms
23:20:57,899 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStart
23:20:57,900 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:20:57,945 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
23:21:05,134 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) This node loaded 10000 entries (~10000000 bytes)
23:21:10,225 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) This node loaded 20000 entries (~20000000 bytes)
23:21:13,884 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) This node loaded 30000 entries (~30000000 bytes)
23:21:14,809 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
23:21:15,039 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
23:21:15,080 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
23:21:15,092 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
23:21:15,130 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
23:21:15,149 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
23:21:15,155 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
23:21:15,162 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
23:21:15,181 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
23:21:15,225 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
23:21:15,226 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
23:21:15,227 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:21:15,437 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
23:21:15,444 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
23:21:15,445 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
23:21:15,445 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
23:21:15,446 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
23:21:15,446 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test warmup
23:21:15,473 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
23:22:15,486 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 1 mins 0 secs
23:22:15,489 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
23:22:15,495 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:22:15,506 INFO  [org.radargun.Slave] (sc-main) Starting stage Clear
23:22:15,509 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) Before executing clear, memory looks like this: 
Runtime free: 1,032,667 kb
Runtime max:1,270,784 kb
Runtime total:1,270,784 kb
MX Code Cache(Non-heap memory): used: 13,191 kb, init: 2,496 kb, committed: 14,016 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 37,897 kb, init: 0 kb, committed: 39,808 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,590 kb, init: 0 kb, committed: 4,992 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 105,772 kb, init: 350,208 kb, committed: 261,120 kb, max: 286,208 kb
MX PS Survivor Space(Heap memory): used: 76,704 kb, init: 57,856 kb, committed: 76,800 kb, max: 76,800 kb
MX PS Old Gen(Heap memory): used: 55,639 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
23:22:15,780 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) After executing clear, memory looks like this: 
Runtime free: 1,277,584 kb
Runtime max:1,291,776 kb
Runtime total:1,291,776 kb
MX Code Cache(Non-heap memory): used: 13,288 kb, init: 2,496 kb, committed: 14,016 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 37,808 kb, init: 0 kb, committed: 39,808 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,553 kb, init: 0 kb, committed: 4,992 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 1,474 kb, init: 350,208 kb, committed: 256,000 kb, max: 258,048 kb
MX PS Survivor Space(Heap memory): used: 0 kb, init: 57,856 kb, committed: 102,912 kb, max: 102,912 kb
MX PS Old Gen(Heap memory): used: 12,717 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
23:22:15,780 INFO  [org.radargun.Slave] (sc-main) Finished stage Clear
23:22:15,780 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:22:15,796 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
23:22:19,178 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) This node loaded 10000 entries (~10000000 bytes)
23:22:22,594 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) This node loaded 20000 entries (~20000000 bytes)
23:22:25,930 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) This node loaded 30000 entries (~30000000 bytes)
23:22:26,920 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
23:22:26,977 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
23:22:26,987 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
23:22:26,993 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
23:22:27,035 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
23:22:27,042 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
23:22:27,044 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
23:22:27,050 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
23:22:27,075 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
23:22:27,086 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
23:22:27,087 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
23:22:27,087 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:22:27,430 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
23:22:27,438 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
23:22:27,439 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
23:22:27,439 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
23:22:27,439 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
23:22:27,439 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test stress-test
23:22:27,472 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
23:32:27,481 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 10 mins 0 secs
23:32:27,483 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
23:32:27,626 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:32:27,805 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStop
23:32:27,807 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStop
23:32:27,808 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:32:27,816 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioDestroy
23:32:27,817 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Scenario finished, destroying...
23:32:27,818 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Memory before cleanup: 
Runtime free: 978,489 kb
Runtime max:1,328,128 kb
Runtime total:1,328,128 kb
MX Code Cache(Non-heap memory): used: 14,747 kb, init: 2,496 kb, committed: 14,912 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 38,377 kb, init: 0 kb, committed: 40,448 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,590 kb, init: 0 kb, committed: 5,120 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 242,408 kb, init: 350,208 kb, committed: 324,608 kb, max: 324,608 kb
MX PS Survivor Space(Heap memory): used: 69,792 kb, init: 57,856 kb, committed: 70,656 kb, max: 70,656 kb
MX PS Old Gen(Heap memory): used: 37,437 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
23:32:27,818 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Stopping service.
23:32:27,837 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-42583: 250+95, fedora-14249: 262+71]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-42583: 265+247, fedora-14249: 247+265]}, unionCH=null, actualMembers=[fedora-42583, fedora-14249], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, b39f6cde-bcd8-4c56-bdac-448952eba963]}
23:32:27,840 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) [Context=testCache][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 11
23:32:27,851 WARN  [org.infinispan.CLUSTER] (remote-thread--p2-t30) [Context=testCache]ISPN000312: Lost data because of graceful leaver fedora-14249
23:32:27,885 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t30) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 132, fedora-12606: 124]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:32:27,886 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t30) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 11
23:32:27,903 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 11
23:32:27,905 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-42583: 126+47, fedora-12606: 130+50]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-42583: 131+125, fedora-12606: 125+131]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:32:27,905 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) [Context=___counters][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 11
23:32:27,906 WARN  [org.infinispan.CLUSTER] (remote-thread--p2-t30) [Context=___counters]ISPN000312: Lost data because of graceful leaver fedora-12606
23:32:27,907 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t15) [Context=___protobuf_metadata][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 12
23:32:27,908 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t16) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___protobuf_metadata, type=REBALANCE_PHASE_CONFIRM, sender=fedora-42583, joinInfo=null, topologyId=11, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-42583 for cache ___protobuf_metadata, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:32:27,908 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t15) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___protobuf_metadata, type=REBALANCE_PHASE_CONFIRM, sender=fedora-42583, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-42583 for cache ___protobuf_metadata, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:32:27,916 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t9) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 11
23:32:27,919 ERROR [org.infinispan.statetransfer.StateConsumerImpl] (transport-thread--p4-t4) ISPN000208: No live owners found for segments {0-1 9-12 19-20 24-29 34 37 42 49-52 56-57 68-69 73-76 80 84-88 98-99 109 113-118 121 130-131 137 146-147 150 153-158 161-164 167-175 183 186 189-191 206 213 225 231 234-235 254-255} of cache ___counters. Excluded owners: []
23:32:27,921 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=___counters][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 12
23:32:27,921 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t30) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 132, fedora-12606: 124]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-42583: 131, fedora-12606: 125]}, unionCH=null, actualMembers=[fedora-42583, fedora-12606], persistentUUIDs=[21c728c3-a322-407f-93bb-929af3084cea, 63e8ea9c-4cc4-48ce-953d-bc30cff2e8aa]}
23:32:27,921 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t24) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counters, type=REBALANCE_PHASE_CONFIRM, sender=fedora-42583, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-42583 for cache ___counters, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:32:27,921 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t30) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100002: Started rebalance with topology id 11
23:32:27,923 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t17) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 11
23:32:27,924 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=org.infinispan.CONFIG][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 12
23:32:27,925 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t19) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=org.infinispan.CONFIG, type=REBALANCE_PHASE_CONFIRM, sender=fedora-42583, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-42583 for cache org.infinispan.CONFIG, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:32:27,924 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=testCache][Scope=fedora-42583]ISPN100003: Node fedora-42583 finished rebalance phase with topology id 12
23:32:27,925 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t17) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=org.infinispan.CONFIG, type=REBALANCE_PHASE_CONFIRM, sender=fedora-42583, joinInfo=null, topologyId=11, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-42583 for cache org.infinispan.CONFIG, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:32:27,937 INFO  [org.infinispan.CLUSTER] (jgroups-32,fedora-42583) ISPN000094: Received new cluster view for channel results: [fedora-42583|3] (2) [fedora-42583, fedora-14249]
23:32:27,937 INFO  [org.infinispan.CLUSTER] (jgroups-32,fedora-42583) ISPN100001: Node fedora-12606 left the cluster
23:32:27,922 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t1) ISPN000210: Failed to request state of cache ___counters from node fedora-12606, segments {0-1 9-12 19-20 24-29 34 37 42 49-52 56-57 68-69 73-76 80 84-88 98-99 109 113-118 121 130-131 137 146-147 150 153-158 161-164 167-175 183 186 189-191 206 213 225 231 234-235 254-255}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-12606 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342-internal]
23:32:27,949 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (StopThread) ISPN000080: Disconnecting JGroups channel results
23:32:27,990 INFO  [org.radargun.service.Infinispan90Lifecycle] (StopThread) Stopped, previous view is [fedora-42583, fedora-12606, fedora-14249]
23:32:27,992 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Service successfully stopped.
23:32:27,992 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioDestroy
23:32:27,993 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
23:32:28,994 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.service.Infinispan80EmbeddedService.destroy() overrides public void org.radargun.service.Infinispan60EmbeddedService.destroy() but both are declared with @Destroy annotation: calling only once
23:32:28,996 INFO  [org.radargun.Slave] (main) Starting stage ScenarioCleanup
23:32:29,101 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Memory after cleanup: 
Runtime free: 1,308,919 kb
Runtime max:1,327,104 kb
Runtime total:1,327,104 kb
MX Code Cache(Non-heap memory): used: 14,825 kb, init: 2,496 kb, committed: 15,104 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 38,497 kb, init: 0 kb, committed: 40,448 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,611 kb, init: 0 kb, committed: 5,120 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 5,659 kb, init: 350,208 kb, committed: 323,584 kb, max: 324,096 kb
MX PS Survivor Space(Heap memory): used: 0 kb, init: 57,856 kb, committed: 70,656 kb, max: 70,656 kb
MX PS Old Gen(Heap memory): used: 12,525 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
23:32:29,103 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
23:32:34,195 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
23:32:38,214 INFO  [org.radargun.Slave] (main) Master shutdown!
23:32:38,215 INFO  [org.radargun.ShutDownHook] (Thread-0) Slave process is being shutdown
