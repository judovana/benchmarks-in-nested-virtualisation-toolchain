13:49:50,307 INFO  [org.radargun.RemoteMasterConnection] (main) Attempting to connect to master results:2103
13:49:50,315 INFO  [org.radargun.RemoteMasterConnection] (main) Successfully established connection with master at: results:2103
13:49:50,317 INFO  [org.radargun.Slave] (main) Received slave index 0
13:49:50,317 INFO  [org.radargun.Slave] (main) Received slave count 3
13:49:50,544 INFO  [org.radargun.ServiceHelper] (sc-main) ServiceContext properties: {}
13:49:50,920 INFO  [org.radargun.Slave] (sc-main) Eager Service Infinispan92EmbeddedService {batching=false, cache=testCache, channelRetrievalTimeout=2 mins 0 secs, enableDiagnostics=null, explicitLocking=false, file=/mnt/workspace/results/dist-sync.xml, internalsExpositionEnabled=false, jgroupsDumperEnabled=false, jgroupsDumperInterval=10.000 secs, keysPerThread=-1, mapReduceDistributedReducePhase=false, mapReduceUseIntermediateSharedCache=false, removedCaches=[  ], threadsPerNode=-1 } loaded.
13:49:53,225 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioInit
13:49:53,270 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioInit
13:49:53,274 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:49:53,361 INFO  [org.radargun.Slave] (sc-main) Starting stage BeforeServiceStart
13:49:53,362 INFO  [org.radargun.Slave] (sc-main) Finished stage BeforeServiceStart
13:49:53,363 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:49:53,371 INFO  [org.radargun.Slave] (sc-main) Starting stage ServiceStart
13:49:53,372 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Startup staggering, this is the slave with index 0, not sleeping
13:49:53,372 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Ack master's StartCluster stage. Local address is: /127.0.0.1. This slave's index is: 0
13:49:53,379 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) Infinispan version: Infinispan 'Gaina' 9.2.0.Final
13:49:53,393 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) JGroups version: JGroups 4.0.10.Final (Schiener Berg)
13:49:54,128 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000078: Starting JGroups channel results
13:49:54,223 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
13:49:54,224 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 20.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
13:49:54,225 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
13:49:54,225 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 25.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
13:49:59,246 INFO  [org.infinispan.CLUSTER] (sc-main) ISPN000094: Received new cluster view for channel results: [fedora-24622|0] (1) [fedora-24622]
13:49:59,337 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000079: Channel results local address is fedora-24622, physical addresses are [192.168.124.63:60328]
13:49:59,340 INFO  [org.infinispan.factories.GlobalComponentRegistry] (sc-main) ISPN000128: Infinispan version: Infinispan 'Gaina' 9.2.0.Final
13:50:00,159 INFO  [org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup] (sc-main) ISPN000107: Retrieving transaction manager Transaction: unknown
13:50:00,334 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
13:50:00,335 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
13:50:00,337 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) ([fedora-24622(local=true, coord=true)]) Number of members=1 is not the one expected: 3
13:50:00,577 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-24622) ISPN000094: Received new cluster view for channel results: [fedora-24622|1] (2) [fedora-24622, fedora-60297]
13:50:00,586 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-24622) ISPN100000: Node fedora-60297 joined the cluster
13:50:00,957 INFO  [org.infinispan.CLUSTER] (jgroups-10,fedora-24622) ISPN000094: Received new cluster view for channel results: [fedora-24622|2] (3) [fedora-24622, fedora-60297, fedora-8475]
13:50:00,962 INFO  [org.infinispan.CLUSTER] (jgroups-10,fedora-24622) ISPN100000: Node fedora-8475 joined the cluster
13:50:01,295 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-24622: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
13:50:01,295 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-24622: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
13:50:01,297 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 2
13:50:01,297 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 2
13:50:01,337 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Number of members is the one expected: 3
13:50:01,338 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Successfully started cache service infinispan92/embedded on slave 0
13:50:01,394 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 2
13:50:01,457 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 2
13:50:01,515 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:641) ~[?:?]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:?]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
13:50:01,556 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:641) ~[?:?]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:?]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
13:50:01,594 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 2
13:50:01,595 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 2
13:50:01,599 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t1) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 3
13:50:01,615 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 2
13:50:01,616 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 2
13:50:01,617 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 3
13:50:01,621 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t4) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 3
13:50:01,622 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 4
13:50:01,625 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 3
13:50:01,635 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 4
13:50:01,636 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 4
13:50:01,657 INFO  [org.radargun.Slave] (sc-main) Finished stage ServiceStart
13:50:01,671 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 4
13:50:01,700 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:50:01,720 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=256, owners = (1)[fedora-24622: 256+0]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-24622: 126+130, fedora-60297: 130+126]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
13:50:01,721 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 2
13:50:01,722 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-24622: 80, fedora-60297: 90, fedora-8475: 86]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297, fedora-8475], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2, 446657fb-ce15-4dc6-b2b2-b9188da82265]}
13:50:01,723 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 6
13:50:01,726 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 6
13:50:01,728 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-24622: 80, fedora-60297: 90, fedora-8475: 86]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297, fedora-8475], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2, 446657fb-ce15-4dc6-b2b2-b9188da82265]}
13:50:01,729 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 6
13:50:01,730 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 2
13:50:01,737 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 6
13:50:01,749 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 6
13:50:01,787 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 6
13:50:01,843 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 2
13:50:01,844 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 2
13:50:01,845 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t5) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 3
13:50:01,861 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 3
13:50:01,867 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t8) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 4
13:50:01,869 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 4
13:50:01,954 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-24622: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
13:50:01,954 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 2
13:50:01,960 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t17) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 2
13:50:01,979 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 2
13:50:01,980 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 2
13:50:01,984 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 3
13:50:01,984 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 3
13:50:01,988 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 4
13:50:02,006 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 4
13:50:02,017 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 6
13:50:02,018 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 6
13:50:02,019 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 7
13:50:02,022 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 7
13:50:02,025 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=512, owners = (1)[fedora-24622: 512+0]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-24622: 256+256, fedora-60297: 256+256]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
13:50:02,026 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 2
13:50:02,037 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t9) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 2
13:50:02,045 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 7
13:50:02,050 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t12) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 8
13:50:02,053 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 8
13:50:02,061 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 8
13:50:02,073 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 2
13:50:02,074 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 2
13:50:02,077 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 3
13:50:02,078 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 3
13:50:02,080 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 4
13:50:02,082 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 4
13:50:02,099 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 6
13:50:02,099 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 6
13:50:02,100 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 7
13:50:02,105 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 7
13:50:02,125 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-24622: 126+130, fedora-60297: 130+126]}, pendingCH=DefaultConsistentHash{ns=256, owners = (3)[fedora-24622: 80+84, fedora-60297: 90+85, fedora-8475: 86+87]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297, fedora-8475], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2, 446657fb-ce15-4dc6-b2b2-b9188da82265]}
13:50:02,128 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 6
13:50:02,128 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 7
13:50:02,130 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t5) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 8
13:50:02,134 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 6
13:50:02,135 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 8
13:50:02,140 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 8
13:50:02,143 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 6
13:50:02,193 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 6
13:50:02,194 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 6
13:50:02,196 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 7
13:50:02,205 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 7
13:50:02,219 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 7
13:50:02,220 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 8
13:50:02,224 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 8
13:50:02,236 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 8
13:50:02,292 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-24622: 80, fedora-60297: 90, fedora-8475: 86]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297, fedora-8475], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2, 446657fb-ce15-4dc6-b2b2-b9188da82265]}
13:50:02,293 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 6
13:50:02,297 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 6
13:50:02,303 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 6
13:50:02,353 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-24622: 256+256, fedora-60297: 256+256]}, pendingCH=DefaultConsistentHash{ns=512, owners = (3)[fedora-24622: 165+180, fedora-60297: 175+171, fedora-8475: 172+161]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297, fedora-8475], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2, 446657fb-ce15-4dc6-b2b2-b9188da82265]}
13:50:02,364 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 6
13:50:02,369 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 6
13:50:02,369 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 6
13:50:02,371 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 7
13:50:02,373 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t4) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 6
13:50:02,380 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 7
13:50:02,381 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 7
13:50:02,381 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 6
13:50:02,383 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t14) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 8
13:50:02,387 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 8
13:50:02,405 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 8
13:50:02,417 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 6
13:50:02,417 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 6
13:50:02,420 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 7
13:50:02,422 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 7
13:50:02,426 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 7
13:50:02,428 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t18) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 8
13:50:02,431 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 8
13:50:02,434 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-8475]ISPN100003: Node fedora-8475 finished rebalance phase with topology id 8
13:50:02,555 INFO  [org.radargun.Slave] (sc-main) Starting stage AfterServiceStart
13:50:02,555 INFO  [org.radargun.Slave] (sc-main) Finished stage AfterServiceStart
13:50:02,558 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:50:02,667 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStart
13:50:02,685 INFO  [org.radargun.sysmonitor.AbstractMonitors] (sc-main) Gathering statistics every 1000 ms
13:50:02,686 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStart
13:50:02,686 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:50:02,725 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
13:50:11,149 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 10000 entries (~10000000 bytes)
13:50:16,353 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) This node loaded 20000 entries (~20000000 bytes)
13:50:20,164 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 30000 entries (~30000000 bytes)
13:50:21,077 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
13:50:21,139 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
13:50:21,214 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
13:50:21,271 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
13:50:21,272 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
13:50:21,302 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
13:50:21,304 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
13:50:21,323 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
13:50:21,331 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
13:50:21,352 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
13:50:21,353 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
13:50:21,353 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:50:21,925 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
13:50:21,928 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
13:50:21,928 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
13:50:21,928 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
13:50:21,929 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
13:50:21,929 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test warmup
13:50:21,941 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
13:51:21,948 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 1 mins 0 secs
13:51:21,951 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
13:51:21,961 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:51:22,000 INFO  [org.radargun.Slave] (sc-main) Starting stage Clear
13:51:22,008 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) Before executing clear, memory looks like this: 
Runtime free: 933,607 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,329 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 43,940 kb, init: 0 kb, committed: 45,436 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 12,648 kb, init: 2,496 kb, committed: 12,672 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 5,012 kb, init: 0 kb, committed: 5,580 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 287,744 kb, init: 73,728 kb, committed: 835,584 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 131,352 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 45,056 kb, init: 0 kb, committed: 45,056 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 4,778 kb, init: 2,496 kb, committed: 4,800 kb, max: 120,032 kb
13:51:22,226 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) After executing clear, memory looks like this: 
Runtime free: 1,381,544 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,335 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 43,922 kb, init: 0 kb, committed: 45,436 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 12,758 kb, init: 2,496 kb, committed: 12,800 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 4,997 kb, init: 0 kb, committed: 5,580 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 0 kb, init: 73,728 kb, committed: 880,640 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 16,727 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 0 kb, init: 0 kb, committed: 0 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 4,794 kb, init: 2,496 kb, committed: 4,800 kb, max: 120,032 kb
13:51:22,228 INFO  [org.radargun.Slave] (sc-main) Finished stage Clear
13:51:22,228 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:51:22,233 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
13:51:25,570 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 10000 entries (~10000000 bytes)
13:51:28,820 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) This node loaded 20000 entries (~20000000 bytes)
13:51:32,029 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) This node loaded 30000 entries (~30000000 bytes)
13:51:33,000 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
13:51:33,006 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
13:51:33,092 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
13:51:33,097 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
13:51:33,126 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
13:51:33,140 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
13:51:33,150 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
13:51:33,153 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
13:51:33,233 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
13:51:33,259 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
13:51:33,259 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
13:51:33,260 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:51:33,537 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
13:51:33,543 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
13:51:33,544 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
13:51:33,545 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
13:51:33,550 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
13:51:33,550 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test stress-test
13:51:33,605 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
14:01:33,611 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 10 mins 0 secs
14:01:33,613 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
14:01:33,702 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
14:01:33,846 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStop
14:01:33,847 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStop
14:01:33,847 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
14:01:33,855 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioDestroy
14:01:33,855 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Scenario finished, destroying...
14:01:33,856 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Memory before cleanup: 
Runtime free: 364,867 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,332 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 44,684 kb, init: 0 kb, committed: 46,204 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 14,643 kb, init: 2,496 kb, committed: 14,656 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 5,038 kb, init: 0 kb, committed: 5,580 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 676,864 kb, init: 73,728 kb, committed: 823,296 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 297,660 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 57,344 kb, init: 0 kb, committed: 57,344 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 5,856 kb, init: 2,496 kb, committed: 5,888 kb, max: 120,032 kb
14:01:33,857 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Stopping service.
14:01:33,880 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-24622: 254+91, fedora-8475: 258+75]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-24622: 255+257, fedora-8475: 257+255]}, unionCH=null, actualMembers=[fedora-24622, fedora-8475], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 446657fb-ce15-4dc6-b2b2-b9188da82265]}
14:01:33,886 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) [Context=testCache][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 11
14:01:33,889 WARN  [org.infinispan.CLUSTER] (remote-thread--p2-t21) [Context=testCache]ISPN000312: Lost data because of graceful leaver fedora-8475
14:01:33,910 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 128, fedora-60297: 128]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
14:01:33,911 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 11
14:01:33,914 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 11
14:01:33,920 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) [Context=___protobuf_metadata][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 11
14:01:33,921 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 11
14:01:33,923 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t13) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 12
14:01:33,924 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t12) [Context=___protobuf_metadata][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 13
14:01:33,926 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t12) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___protobuf_metadata, type=REBALANCE_PHASE_CONFIRM, sender=fedora-24622, joinInfo=null, topologyId=13, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-24622 for cache ___protobuf_metadata, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:33,930 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t13) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___protobuf_metadata, type=REBALANCE_PHASE_CONFIRM, sender=fedora-24622, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-24622 for cache ___protobuf_metadata, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:33,937 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-24622: 123+41, fedora-60297: 133+42]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-24622: 126+130, fedora-60297: 130+126]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
14:01:33,938 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) [Context=___counters][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 11
14:01:33,946 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 128, fedora-60297: 128]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
14:01:33,947 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 11
14:01:33,949 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 11
14:01:33,951 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) [Context=org.infinispan.CONFIG][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 11
14:01:33,954 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=12, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 128, fedora-60297: 128]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-24622: 126, fedora-60297: 130]}, unionCH=null, actualMembers=[fedora-24622, fedora-60297], persistentUUIDs=[c1d06402-1ec6-45e6-9962-ecd5ad14619c, 404cee4c-ace4-4270-aa1b-1fbb93df4fd2]}
14:01:33,955 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) [Context=___counter_configuration][Scope=fedora-24622]ISPN100002: Started rebalance with topology id 12
14:01:33,956 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 12
14:01:33,958 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 12
14:01:33,960 WARN  [org.infinispan.CLUSTER] (remote-thread--p2-t26) [Context=___counters]ISPN000312: Lost data because of graceful leaver fedora-60297
14:01:33,978 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t19) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 12
14:01:33,980 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 13
14:01:33,979 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 11
14:01:33,983 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 12
14:01:33,996 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t3) [Context=testCache][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 12
14:01:33,980 ERROR [org.infinispan.statetransfer.StateConsumerImpl] (transport-thread--p4-t16) ISPN000208: No live owners found for segments {0-1 7-8 21-23 36-37 44 50-52 56-59 73-75 82-84 88-95 102-107 114-116 119 138-139 144-146 155-157 165-169 172-173 181-182 185-188 191-203 210-212 216-220 224-225 232-233 237 245 248-250} of cache ___counters. Excluded owners: []
14:01:34,002 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=___counters][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 12
14:01:34,004 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t16) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counters, type=REBALANCE_PHASE_CONFIRM, sender=fedora-24622, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-24622 for cache ___counters, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:33,998 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t10) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=org.infinispan.CONFIG, type=REBALANCE_PHASE_CONFIRM, sender=fedora-24622, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-24622 for cache org.infinispan.CONFIG, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:33,981 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t2) ISPN000210: Failed to request state of cache ___counters from node fedora-60297, segments {0-1 7-8 21-23 36-37 44 50-52 56-59 73-75 82-84 88-95 102-107 114-116 119 138-139 144-146 155-157 165-169 172-173 181-182 185-188 191-203 210-212 216-220 224-225 232-233 237 245 248-250}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-60297 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:33,989 INFO  [org.infinispan.CLUSTER] (jgroups-33,fedora-24622) ISPN000094: Received new cluster view for channel results: [fedora-24622|3] (2) [fedora-24622, fedora-60297]
14:01:33,983 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t21) [Context=___counter_configuration][Scope=fedora-60297]ISPN100003: Node fedora-60297 finished rebalance phase with topology id 13
14:01:34,017 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t14) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 14
14:01:34,017 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t17) [Context=___counter_configuration][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 15
14:01:34,016 INFO  [org.infinispan.CLUSTER] (jgroups-33,fedora-24622) ISPN100001: Node fedora-8475 left the cluster
14:01:33,984 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=org.infinispan.CONFIG][Scope=fedora-24622]ISPN100003: Node fedora-24622 finished rebalance phase with topology id 13
14:01:33,996 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t1) ISPN000210: Failed to request state of cache testCache from node fedora-8475, segments {1-3 8-10 14 33-34 42-46 49 54 60-62 73-75 84 100-101 104-106 109 128 132 139 146-148 164 169 174-175 180-193 198 203 206-208 211-215 220-224 227-228 232-234 238-239 244-245 253-256 265 269-270 277-279 282 294 299-300 303 310-316 330-339 353-356 363-367 370 376 382-383 387 391-392 396-404 408 414 420-425 432 435 441-442 445 450-454 457 465-468 474 490 495-496 499-501 506-508}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-8475 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:34,017 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t21) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counter_configuration, type=REBALANCE_PHASE_CONFIRM, sender=fedora-60297, joinInfo=null, topologyId=13, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-60297 for cache ___counter_configuration, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
14:01:34,028 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (StopThread) ISPN000080: Disconnecting JGroups channel results
14:01:34,053 INFO  [org.radargun.service.Infinispan90Lifecycle] (StopThread) Stopped, previous view is [fedora-24622, fedora-60297, fedora-8475]
14:01:34,054 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Service successfully stopped.
14:01:34,054 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioDestroy
14:01:34,055 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
14:01:35,149 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.service.Infinispan80EmbeddedService.destroy() overrides public void org.radargun.service.Infinispan60EmbeddedService.destroy() but both are declared with @Destroy annotation: calling only once
14:01:35,152 INFO  [org.radargun.Slave] (main) Starting stage ScenarioCleanup
14:01:35,156 WARN  [org.radargun.stages.ScenarioCleanupStage] (main) Unfinished thread ForkJoinPool.commonPool-worker-3 (id=34, state=WAITING)
	at java.base@11.0.12-internal/jdk.internal.misc.Unsafe.park(Native Method)
	at java.base@11.0.12-internal/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
	at java.base@11.0.12-internal/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
	at java.base@11.0.12-internal/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
14:01:35,158 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Interrupting thread ForkJoinPool.commonPool-worker-3 (id=34, state=WAITING)
14:01:40,160 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Stopping thread ForkJoinPool.commonPool-worker-3 (id=34, state=WAITING)
14:01:40,169 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Is thread ForkJoinPool.commonPool-worker-3 (id=34, state=TERMINATED)) alive? false
14:01:40,303 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Memory after cleanup: 
Runtime free: 1,380,627 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,334 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 44,824 kb, init: 0 kb, committed: 46,204 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 14,750 kb, init: 2,496 kb, committed: 14,784 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 5,060 kb, init: 0 kb, committed: 5,580 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 0 kb, init: 73,728 kb, committed: 880,640 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 17,644 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 0 kb, init: 0 kb, committed: 0 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 5,951 kb, init: 2,496 kb, committed: 5,952 kb, max: 120,032 kb
14:01:40,305 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
14:01:40,349 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
14:01:50,357 INFO  [org.radargun.ShutDownHook] (Thread-0) Slave process is being shutdown
