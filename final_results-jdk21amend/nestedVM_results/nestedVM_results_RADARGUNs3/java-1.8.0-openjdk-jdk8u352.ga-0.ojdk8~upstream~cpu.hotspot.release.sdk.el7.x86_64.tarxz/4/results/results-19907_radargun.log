13:55:10,867 INFO  [org.radargun.RemoteMasterConnection] (main) Attempting to connect to master results:2103
13:55:10,872 INFO  [org.radargun.RemoteMasterConnection] (main) Successfully established connection with master at: results:2103
13:55:10,889 INFO  [org.radargun.Slave] (main) Received slave index 0
13:55:10,892 INFO  [org.radargun.Slave] (main) Received slave count 3
13:55:11,073 INFO  [org.radargun.ServiceHelper] (sc-main) ServiceContext properties: {}
13:55:11,189 INFO  [org.radargun.Slave] (sc-main) Eager Service Infinispan92EmbeddedService {batching=false, cache=testCache, channelRetrievalTimeout=2 mins 0 secs, enableDiagnostics=null, explicitLocking=false, file=/mnt/workspace/results/dist-sync.xml, internalsExpositionEnabled=false, jgroupsDumperEnabled=false, jgroupsDumperInterval=10.000 secs, keysPerThread=-1, mapReduceDistributedReducePhase=false, mapReduceUseIntermediateSharedCache=false, removedCaches=[  ], threadsPerNode=-1 } loaded.
13:55:13,257 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioInit
13:55:13,935 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioInit
13:55:13,963 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:55:13,968 INFO  [org.radargun.Slave] (sc-main) Starting stage BeforeServiceStart
13:55:13,968 INFO  [org.radargun.Slave] (sc-main) Finished stage BeforeServiceStart
13:55:13,968 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:55:13,982 INFO  [org.radargun.Slave] (sc-main) Starting stage ServiceStart
13:55:13,982 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Startup staggering, this is the slave with index 0, not sleeping
13:55:13,983 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Ack master's StartCluster stage. Local address is: /127.0.0.1. This slave's index is: 0
13:55:13,986 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) Infinispan version: Infinispan 'Gaina' 9.2.0.Final
13:55:13,998 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) JGroups version: JGroups 4.0.10.Final (Schiener Berg)
13:55:14,569 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000078: Starting JGroups channel results
13:55:14,659 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
13:55:14,659 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 20.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
13:55:14,660 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
13:55:14,660 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 25.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
13:55:19,702 INFO  [org.infinispan.CLUSTER] (sc-main) ISPN000094: Received new cluster view for channel results: [fedora-62593|0] (1) [fedora-62593]
13:55:19,772 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000079: Channel results local address is fedora-62593, physical addresses are [192.168.124.213:46987]
13:55:19,774 INFO  [org.infinispan.factories.GlobalComponentRegistry] (sc-main) ISPN000128: Infinispan version: Infinispan 'Gaina' 9.2.0.Final
13:55:20,549 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-62593) ISPN000094: Received new cluster view for channel results: [fedora-62593|1] (2) [fedora-62593, fedora-42498]
13:55:20,585 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-62593) ISPN100000: Node fedora-42498 joined the cluster
13:55:20,639 INFO  [org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup] (sc-main) ISPN000107: Retrieving transaction manager Transaction: unknown
13:55:20,832 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
13:55:20,832 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
13:55:20,833 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) ([fedora-62593(local=true, coord=true), fedora-42498(local=false, coord=false)]) Number of members=2 is not the one expected: 3
13:55:21,273 INFO  [org.infinispan.CLUSTER] (jgroups-11,fedora-62593) ISPN000094: Received new cluster view for channel results: [fedora-62593|2] (3) [fedora-62593, fedora-42498, fedora-20331]
13:55:21,275 INFO  [org.infinispan.CLUSTER] (jgroups-11,fedora-62593) ISPN100000: Node fedora-20331 joined the cluster
13:55:21,447 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-62593: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 134, fedora-42498: 122]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37]}
13:55:21,447 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-62593: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 134, fedora-42498: 122]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37]}
13:55:21,448 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 2
13:55:21,452 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 2
13:55:21,485 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 2
13:55:21,521 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 2
13:55:21,708 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 2
13:55:21,716 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 2
13:55:21,716 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 2
13:55:21,722 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 2
13:55:21,726 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t1) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 3
13:55:21,726 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 3
13:55:21,755 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 3
13:55:21,761 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 4
13:55:21,770 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 3
13:55:21,773 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 4
13:55:21,795 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 4
13:55:21,799 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 4
13:55:21,834 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Number of members is the one expected: 3
13:55:21,834 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Successfully started cache service infinispan92/embedded on slave 0
13:55:21,864 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=256, owners = (1)[fedora-62593: 256+0]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-62593: 134+122, fedora-42498: 122+134]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37]}
13:55:21,864 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 2
13:55:21,870 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=___counters][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 2
13:55:21,905 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[?:1.8.0_352-internal]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:1.8.0_352-internal]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
13:55:21,925 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[?:1.8.0_352-internal]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:1.8.0_352-internal]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
13:55:21,993 INFO  [org.radargun.Slave] (sc-main) Finished stage ServiceStart
13:55:22,032 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:55:22,087 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-62593: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 134, fedora-42498: 122]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37]}
13:55:22,088 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 2
13:55:22,094 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t5) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 2
13:55:22,116 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 2
13:55:22,117 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 2
13:55:22,120 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___counters][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 3
13:55:22,133 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 2
13:55:22,134 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 2
13:55:22,139 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t8) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 3
13:55:22,140 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 3
13:55:22,144 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=___counters][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 4
13:55:22,156 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 3
13:55:22,158 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 4
13:55:22,167 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=512, owners = (1)[fedora-62593: 512+0]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-62593: 265+247, fedora-42498: 247+265]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37]}
13:55:22,168 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 2
13:55:22,171 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 4
13:55:22,179 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=testCache][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 2
13:55:22,180 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 4
13:55:22,201 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 134, fedora-42498: 122]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-62593: 81, fedora-42498: 85, fedora-20331: 90]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37, a635883c-f36d-4395-88ba-df71de2053a0]}
13:55:22,202 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 6
13:55:22,209 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 6
13:55:22,215 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 134, fedora-42498: 122]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-62593: 81, fedora-42498: 85, fedora-20331: 90]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37, a635883c-f36d-4395-88ba-df71de2053a0]}
13:55:22,215 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 6
13:55:22,218 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 6
13:55:22,228 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 2
13:55:22,229 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 2
13:55:22,231 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t8) [Context=testCache][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 3
13:55:22,235 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 3
13:55:22,241 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t13) [Context=testCache][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 4
13:55:22,241 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 6
13:55:22,244 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 4
13:55:22,251 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=org.infinispan.CONFIG][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 6
13:55:22,470 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 6
13:55:22,472 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 6
13:55:22,473 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 7
13:55:22,477 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 7
13:55:22,482 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 7
13:55:22,485 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 8
13:55:22,486 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 8
13:55:22,496 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___counter_configuration][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 8
13:55:22,498 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 6
13:55:22,499 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 6
13:55:22,504 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 7
13:55:22,505 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 7
13:55:22,514 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 7
13:55:22,515 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t9) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 8
13:55:22,517 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 8
13:55:22,534 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 8
13:55:22,551 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-62593: 134+122, fedora-42498: 122+134]}, pendingCH=DefaultConsistentHash{ns=256, owners = (3)[fedora-62593: 81+75, fedora-42498: 85+93, fedora-20331: 90+88]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37, a635883c-f36d-4395-88ba-df71de2053a0]}
13:55:22,552 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 6
13:55:22,555 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 6
13:55:22,556 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=___counters][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 6
13:55:22,628 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 6
13:55:22,628 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 6
13:55:22,630 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___counters][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 7
13:55:22,632 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 7
13:55:22,636 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 7
13:55:22,638 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t15) [Context=___counters][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 8
13:55:22,640 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 8
13:55:22,644 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 8
13:55:22,652 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 134, fedora-42498: 122]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-62593: 81, fedora-42498: 85, fedora-20331: 90]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37, a635883c-f36d-4395-88ba-df71de2053a0]}
13:55:22,654 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 6
13:55:22,661 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t1) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 6
13:55:22,664 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 6
13:55:22,690 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 6
13:55:22,690 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 6
13:55:22,693 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 7
13:55:22,696 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 7
13:55:22,697 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 7
13:55:22,698 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t14) [Context=___protobuf_metadata][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 8
13:55:22,703 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t3) [Context=___protobuf_metadata][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 8
13:55:22,703 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 8
13:55:22,722 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-62593: 265+247, fedora-42498: 247+265]}, pendingCH=DefaultConsistentHash{ns=512, owners = (3)[fedora-62593: 172+187, fedora-42498: 160+158, fedora-20331: 180+167]}, unionCH=null, actualMembers=[fedora-62593, fedora-42498, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, 884100ea-4ee2-4ab8-8017-8e8f4f2fdf37, a635883c-f36d-4395-88ba-df71de2053a0]}
13:55:22,723 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 6
13:55:22,726 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t16) [Context=testCache][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 6
13:55:22,732 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 6
13:55:22,752 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 6
13:55:22,753 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 6
13:55:22,755 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=testCache][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 7
13:55:22,759 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 7
13:55:22,760 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 7
13:55:22,763 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=testCache][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 8
13:55:22,767 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-42498]ISPN100003: Node fedora-42498 finished rebalance phase with topology id 8
13:55:22,770 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 8
13:55:22,904 INFO  [org.radargun.Slave] (sc-main) Starting stage AfterServiceStart
13:55:22,905 INFO  [org.radargun.Slave] (sc-main) Finished stage AfterServiceStart
13:55:22,909 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:55:22,989 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStart
13:55:22,999 INFO  [org.radargun.sysmonitor.AbstractMonitors] (sc-main) Gathering statistics every 1000 ms
13:55:22,999 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStart
13:55:23,004 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:55:23,041 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
13:55:31,069 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) This node loaded 10000 entries (~10000000 bytes)
13:55:35,380 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) This node loaded 20000 entries (~20000000 bytes)
13:55:39,239 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) This node loaded 30000 entries (~30000000 bytes)
13:55:40,171 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
13:55:40,283 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
13:55:40,322 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
13:55:40,324 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
13:55:40,335 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
13:55:40,337 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
13:55:40,361 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
13:55:40,413 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
13:55:40,423 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
13:55:40,423 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
13:55:40,424 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
13:55:40,425 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:55:40,550 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
13:55:40,553 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
13:55:40,554 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
13:55:40,554 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
13:55:40,554 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
13:55:40,554 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test warmup
13:55:40,571 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
13:56:40,589 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 1 mins 0 secs
13:56:40,591 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
13:56:40,601 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:56:40,621 INFO  [org.radargun.Slave] (sc-main) Starting stage Clear
13:56:40,626 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) Before executing clear, memory looks like this: 
Runtime free: 895,478 kb
Runtime max:1,266,688 kb
Runtime total:1,266,688 kb
MX Code Cache(Non-heap memory): used: 13,382 kb, init: 2,496 kb, committed: 14,336 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 37,908 kb, init: 0 kb, committed: 39,808 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,586 kb, init: 0 kb, committed: 4,992 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 232,770 kb, init: 350,208 kb, committed: 253,952 kb, max: 279,040 kb
MX PS Survivor Space(Heap memory): used: 79,424 kb, init: 57,856 kb, committed: 79,872 kb, max: 79,872 kb
MX PS Old Gen(Heap memory): used: 59,014 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
13:56:40,847 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) After executing clear, memory looks like this: 
Runtime free: 1,276,038 kb
Runtime max:1,289,728 kb
Runtime total:1,289,728 kb
MX Code Cache(Non-heap memory): used: 13,465 kb, init: 2,496 kb, committed: 14,336 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 37,835 kb, init: 0 kb, committed: 39,808 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,555 kb, init: 0 kb, committed: 4,992 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 987 kb, init: 350,208 kb, committed: 249,856 kb, max: 250,880 kb
MX PS Survivor Space(Heap memory): used: 0 kb, init: 57,856 kb, committed: 107,008 kb, max: 107,008 kb
MX PS Old Gen(Heap memory): used: 12,714 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
13:56:40,848 INFO  [org.radargun.Slave] (sc-main) Finished stage Clear
13:56:40,849 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:56:40,929 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
13:56:44,134 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) This node loaded 10000 entries (~10000000 bytes)
13:56:47,295 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) This node loaded 20000 entries (~20000000 bytes)
13:56:50,553 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 30000 entries (~30000000 bytes)
13:56:51,450 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
13:56:51,519 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
13:56:51,578 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
13:56:51,591 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
13:56:51,646 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
13:56:51,664 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
13:56:51,678 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
13:56:51,688 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
13:56:51,706 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
13:56:51,727 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
13:56:51,728 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
13:56:51,728 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
13:56:52,303 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
13:56:52,318 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
13:56:52,319 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
13:56:52,319 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
13:56:52,320 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
13:56:52,320 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test stress-test
13:56:52,332 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
14:06:52,335 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 10 mins 0 secs
14:06:52,337 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
14:06:52,458 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
14:06:52,762 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStop
14:06:52,764 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStop
14:06:52,765 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
14:06:52,774 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioDestroy
14:06:52,774 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Scenario finished, destroying...
14:06:52,775 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Memory before cleanup: 
Runtime free: 913,628 kb
Runtime max:1,325,056 kb
Runtime total:1,325,056 kb
MX Code Cache(Non-heap memory): used: 14,951 kb, init: 2,496 kb, committed: 15,104 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 38,414 kb, init: 0 kb, committed: 40,448 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,591 kb, init: 0 kb, committed: 5,120 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 298,656 kb, init: 350,208 kb, committed: 318,464 kb, max: 318,464 kb
MX PS Survivor Space(Heap memory): used: 72,928 kb, init: 57,856 kb, committed: 73,728 kb, max: 73,728 kb
MX PS Old Gen(Heap memory): used: 39,842 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
14:06:52,775 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Stopping service.
14:06:52,810 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-62593: 262+97, fedora-20331: 250+97]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-62593: 257+255, fedora-20331: 255+257]}, unionCH=null, actualMembers=[fedora-62593, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, a635883c-f36d-4395-88ba-df71de2053a0]}
14:06:52,815 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) [Context=testCache][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 11
14:06:52,817 WARN  [org.infinispan.CLUSTER] (remote-thread--p2-t15) [Context=testCache]ISPN000312: Lost data because of graceful leaver fedora-20331
14:06:52,888 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t15) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-62593: 124+32, fedora-20331: 132+46]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-62593: 122+134, fedora-20331: 134+122]}, unionCH=null, actualMembers=[fedora-62593, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, a635883c-f36d-4395-88ba-df71de2053a0]}
14:06:52,890 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t15) [Context=___counters][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 11
14:06:52,892 WARN  [org.infinispan.CLUSTER] (StopThread) [Context=___counters]ISPN000312: Lost data because of graceful leaver fedora-62593
14:06:52,899 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t15) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 126, fedora-20331: 130]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 122, fedora-20331: 134]}, unionCH=null, actualMembers=[fedora-62593, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, a635883c-f36d-4395-88ba-df71de2053a0]}
14:06:52,899 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t15) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 11
14:06:52,902 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 11
14:06:52,910 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t15) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=12, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 126, fedora-20331: 130]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-62593: 122, fedora-20331: 134]}, unionCH=null, actualMembers=[fedora-62593, fedora-20331], persistentUUIDs=[9fdde4f4-92d3-4b42-a4ed-678306bfb68f, a635883c-f36d-4395-88ba-df71de2053a0]}
14:06:52,911 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t15) [Context=___counter_configuration][Scope=fedora-62593]ISPN100002: Started rebalance with topology id 12
14:06:52,913 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t21) [Context=___counter_configuration][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 12
14:06:52,902 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t1) ISPN000210: Failed to request state of cache testCache from node fedora-20331, segments {8-10 16-17 25-26 36-39 43-44 56-57 82-83 95 98-101 105-110 115 122 130-135 146-150 161 164-166 172-175 180-181 186-189 193-200 203 209-211 215 222-231 235-236 249 258 263 267 271-272 279-280 285-288 291-292 296-300 306-307 314-316 322-325 331-332 339-346 349-360 368 385-386 389-393 415-416 421-422 435 439-440 458 485 489-490 500-502 506}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-20331 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352-internal]
14:06:52,950 INFO  [org.infinispan.CLUSTER] (jgroups-27,fedora-62593) ISPN000094: Received new cluster view for channel results: [fedora-62593|3] (2) [fedora-62593, fedora-20331]
14:06:52,955 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=org.infinispan.CONFIG][Scope=fedora-62593]ISPN100003: Node fedora-62593 finished rebalance phase with topology id 12
14:06:52,962 INFO  [org.infinispan.CLUSTER] (jgroups-27,fedora-62593) ISPN100001: Node fedora-42498 left the cluster
14:06:52,951 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t31) [Context=___counter_configuration][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 12
14:06:52,951 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t26) [Context=org.infinispan.CONFIG][Scope=fedora-20331]ISPN100003: Node fedora-20331 finished rebalance phase with topology id 11
14:06:52,965 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (remote-thread--p2-t31) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counter_configuration, type=REBALANCE_PHASE_CONFIRM, sender=fedora-20331, joinInfo=null, topologyId=12, rebalanceId=0, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-20331 for cache ___counter_configuration, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.invokeReplicableCommand(GlobalInboundInvocationHandler.java:169) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.runReplicableCommand(GlobalInboundInvocationHandler.java:150) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.inboundhandler.GlobalInboundInvocationHandler.lambda$handleReplicableCommand$1(GlobalInboundInvocationHandler.java:144) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.util.concurrent.BlockingTaskAwareExecutorServiceImpl$RunnableWrapper.run(BlockingTaskAwareExecutorServiceImpl.java:212) [infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352-internal]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352-internal]
14:06:52,971 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (StopThread) ISPN000080: Disconnecting JGroups channel results
14:06:53,018 INFO  [org.radargun.service.Infinispan90Lifecycle] (StopThread) Stopped, previous view is [fedora-62593, fedora-42498, fedora-20331]
14:06:53,019 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Service successfully stopped.
14:06:53,019 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioDestroy
14:06:53,019 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
14:06:54,127 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.service.Infinispan80EmbeddedService.destroy() overrides public void org.radargun.service.Infinispan60EmbeddedService.destroy() but both are declared with @Destroy annotation: calling only once
14:06:54,134 INFO  [org.radargun.Slave] (main) Starting stage ScenarioCleanup
14:06:54,256 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Memory after cleanup: 
Runtime free: 1,305,149 kb
Runtime max:1,324,544 kb
Runtime total:1,324,544 kb
MX Code Cache(Non-heap memory): used: 15,094 kb, init: 2,496 kb, committed: 15,296 kb, max: 245,760 kb
MX Metaspace(Non-heap memory): used: 38,520 kb, init: 0 kb, committed: 40,704 kb, max: 0 kb
MX Compressed Class Space(Non-heap memory): used: 4,612 kb, init: 0 kb, committed: 5,120 kb, max: 1,048,576 kb
MX PS Eden Space(Heap memory): used: 6,821 kb, init: 350,208 kb, committed: 317,440 kb, max: 317,440 kb
MX PS Survivor Space(Heap memory): used: 0 kb, init: 57,856 kb, committed: 74,240 kb, max: 74,240 kb
MX PS Old Gen(Heap memory): used: 12,572 kb, init: 932,864 kb, committed: 932,864 kb, max: 932,864 kb
14:06:54,259 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
14:06:59,417 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
14:07:03,788 INFO  [org.radargun.Slave] (main) Master shutdown!
