19:54:20,286 INFO  [org.radargun.RemoteMasterConnection] (main) Attempting to connect to master results:2103
19:54:20,295 INFO  [org.radargun.RemoteMasterConnection] (main) Successfully established connection with master at: results:2103
19:54:20,298 INFO  [org.radargun.Slave] (main) Received slave index 0
19:54:20,299 INFO  [org.radargun.Slave] (main) Received slave count 3
19:54:20,681 INFO  [org.radargun.ServiceHelper] (sc-main) ServiceContext properties: {}
19:54:21,054 INFO  [org.radargun.Slave] (sc-main) Eager Service Infinispan92EmbeddedService {batching=false, cache=testCache, channelRetrievalTimeout=2 mins 0 secs, enableDiagnostics=null, explicitLocking=false, file=/mnt/workspace/results/dist-sync.xml, internalsExpositionEnabled=false, jgroupsDumperEnabled=false, jgroupsDumperInterval=10.000 secs, keysPerThread=-1, mapReduceDistributedReducePhase=false, mapReduceUseIntermediateSharedCache=false, removedCaches=[  ], threadsPerNode=-1 } loaded.
19:54:23,078 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioInit
19:54:23,249 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioInit
19:54:23,252 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:54:23,329 INFO  [org.radargun.Slave] (sc-main) Starting stage BeforeServiceStart
19:54:23,329 INFO  [org.radargun.Slave] (sc-main) Finished stage BeforeServiceStart
19:54:23,330 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:54:23,337 INFO  [org.radargun.Slave] (sc-main) Starting stage ServiceStart
19:54:23,338 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Startup staggering, this is the slave with index 0, not sleeping
19:54:23,339 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Ack master's StartCluster stage. Local address is: /127.0.0.1. This slave's index is: 0
19:54:23,344 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) Infinispan version: Infinispan 'Gaina' 9.2.0.Final
19:54:23,359 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) JGroups version: JGroups 4.0.10.Final (Schiener Berg)
19:54:24,331 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000078: Starting JGroups channel results
19:54:24,444 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
19:54:24,445 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 20.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
19:54:24,446 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the send buffer of socket MulticastSocket was set to 1.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
19:54:24,447 WARN  [org.jgroups.protocols.UDP] (sc-main) JGRP000015: the receive buffer of socket MulticastSocket was set to 25.00MB, but the OS only allocated 212.99KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
19:54:29,476 INFO  [org.infinispan.CLUSTER] (sc-main) ISPN000094: Received new cluster view for channel results: [fedora-7911|0] (1) [fedora-7911]
19:54:29,580 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (sc-main) ISPN000079: Channel results local address is fedora-7911, physical addresses are [192.168.124.176:47941]
19:54:29,595 INFO  [org.infinispan.factories.GlobalComponentRegistry] (sc-main) ISPN000128: Infinispan version: Infinispan 'Gaina' 9.2.0.Final
19:54:30,421 INFO  [org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup] (sc-main) ISPN000107: Retrieving transaction manager Transaction: unknown
19:54:30,564 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-7911) ISPN000094: Received new cluster view for channel results: [fedora-7911|1] (2) [fedora-7911, fedora-23681]
19:54:30,568 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
19:54:30,568 INFO  [org.radargun.service.Infinispan90Lifecycle] (sc-main) No RELAY2 protocol in XS service
19:54:30,569 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) ([fedora-7911(local=true, coord=true), fedora-23681(local=false, coord=false)]) Number of members=2 is not the one expected: 3
19:54:30,580 INFO  [org.infinispan.CLUSTER] (jgroups-6,fedora-7911) ISPN100000: Node fedora-23681 joined the cluster
19:54:30,858 INFO  [org.infinispan.CLUSTER] (jgroups-10,fedora-7911) ISPN000094: Received new cluster view for channel results: [fedora-7911|2] (3) [fedora-7911, fedora-23681, fedora-14710]
19:54:30,861 INFO  [org.infinispan.CLUSTER] (jgroups-10,fedora-7911) ISPN100000: Node fedora-14710 joined the cluster
19:54:31,204 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-7911: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-7911: 133, fedora-23681: 123]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6]}
19:54:31,207 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 2
19:54:31,217 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-7911: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-7911: 133, fedora-23681: 123]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6]}
19:54:31,224 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 2
19:54:31,276 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t23) [Context=___counter_configuration][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 2
19:54:31,279 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 2
19:54:31,397 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 2
19:54:31,398 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 2
19:54:31,406 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=___counter_configuration][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 3
19:54:31,423 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=org.infinispan.CONFIG][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 2
19:54:31,424 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 2
19:54:31,425 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 3
19:54:31,426 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t5) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 3
19:54:31,429 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t8) [Context=___counter_configuration][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 4
19:54:31,433 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 3
19:54:31,435 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 4
19:54:31,445 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 4
19:54:31,452 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 4
19:54:31,496 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=256, owners = (1)[fedora-7911: 256+0]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-7911: 133+123, fedora-23681: 123+133]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6]}
19:54:31,497 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 2
19:54:31,501 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 2
19:54:31,568 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache org.infinispan.CONFIG, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-7911: 133, fedora-23681: 123]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-7911: 90, fedora-23681: 84, fedora-14710: 82]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6, d6f750e4-03ef-442f-808f-9f13da616157]}
19:54:31,569 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 6
19:54:31,570 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Number of members is the one expected: 3
19:54:31,570 INFO  [org.radargun.stages.lifecycle.ServiceStartStage] (sc-main) Successfully started cache service infinispan92/embedded on slave 0
19:54:31,582 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 6
19:54:31,588 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counter_configuration, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-7911: 133, fedora-23681: 123]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-7911: 90, fedora-23681: 84, fedora-14710: 82]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6, d6f750e4-03ef-442f-808f-9f13da616157]}
19:54:31,589 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 6
19:54:31,594 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t2) [Context=___counter_configuration][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 6
19:54:31,599 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 6
19:54:31,632 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 2
19:54:31,634 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 2
19:54:31,635 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counter_configuration][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 6
19:54:31,636 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t7) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 3
19:54:31,652 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 3
19:54:31,658 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 4
19:54:31,686 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 4
19:54:31,712 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:641) ~[?:?]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:?]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
19:54:31,767 ERROR [org.radargun.service.ConfigDumpHelper] (sc-main) Error while dumping ___protobuf_metadata cache config as properties
javax.management.AttributeNotFoundException: Unknown attribute 'configurationAsProperties'. Known attributes names are: [rebalancingEnabled, cacheStatus, cacheName, cacheAvailability, version, configurationAsProperties]
	at org.infinispan.jmx.ResourceDMBean.getAttribute(ResourceDMBean.java:181) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:641) ~[?:?]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[?:?]
	at org.radargun.service.ConfigDumpHelper60.dumpCache(ConfigDumpHelper60.java:44) [plugin-infinispan60-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.service.EmbeddedConfigurationProvider.getNormalizedConfigs(EmbeddedConfigurationProvider.java:33) [plugin-infinispan52-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.stages.lifecycle.ServiceStartStage.executeOnSlave(ServiceStartStage.java:92) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase.scenarioLoop(SlaveBase.java:102) [radargun-core-3.0.0-SNAPSHOT.jar:?]
	at org.radargun.SlaveBase$ScenarioRunner.run(SlaveBase.java:203) [radargun-core-3.0.0-SNAPSHOT.jar:?]
19:54:31,806 INFO  [org.radargun.Slave] (sc-main) Finished stage ServiceStart
19:54:31,836 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:54:31,848 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=ReplicatedConsistentHash{ns = 256, owners = (1)[fedora-7911: 256]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-7911: 133, fedora-23681: 123]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6]}
19:54:31,848 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 2
19:54:31,859 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 2
19:54:31,910 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=2, phase=READ_OLD_WRITE_ALL, rebalanceId=2, currentCH=DefaultConsistentHash{ns=512, owners = (1)[fedora-7911: 512+0]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-7911: 262+250, fedora-23681: 250+262]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6]}
19:54:31,911 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 2
19:54:31,917 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t18) [Context=testCache][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 2
19:54:31,958 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 2
19:54:31,959 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 2
19:54:31,961 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 3
19:54:31,963 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t25) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 3
19:54:31,965 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t1) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 4
19:54:31,968 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___protobuf_metadata][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 4
19:54:32,008 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 2
19:54:32,009 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 2
19:54:32,011 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t12) [Context=testCache][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 3
19:54:32,020 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 3
19:54:32,023 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=testCache][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 4
19:54:32,027 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=testCache][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 4
19:54:32,038 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 6
19:54:32,039 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache ___counter_configuration, topology id = 6
19:54:32,042 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t18) [Context=___counter_configuration][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 7
19:54:32,042 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 7
19:54:32,084 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 7
19:54:32,086 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t22) [Context=___counter_configuration][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 8
19:54:32,088 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 8
19:54:32,110 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counter_configuration][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 8
19:54:32,118 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 6
19:54:32,120 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000336: Finished cluster-wide rebalance for cache org.infinispan.CONFIG, topology id = 6
19:54:32,124 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t5) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 7
19:54:32,127 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 7
19:54:32,136 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 7
19:54:32,140 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t10) [Context=org.infinispan.CONFIG][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 8
19:54:32,145 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 8
19:54:32,159 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=org.infinispan.CONFIG][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 8
19:54:32,170 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-7911: 133+123, fedora-23681: 123+133]}, pendingCH=DefaultConsistentHash{ns=256, owners = (3)[fedora-7911: 90+98, fedora-23681: 84+80, fedora-14710: 82+78]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6, d6f750e4-03ef-442f-808f-9f13da616157]}
19:54:32,171 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 6
19:54:32,174 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t19) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 6
19:54:32,180 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 6
19:54:32,249 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 6
19:54:32,250 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___counters, topology id = 6
19:54:32,252 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 7
19:54:32,255 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 7
19:54:32,268 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 7
19:54:32,270 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 8
19:54:32,276 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t1) [Context=___counters][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 8
19:54:32,276 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___counters][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 8
19:54:32,321 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache ___protobuf_metadata, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=ReplicatedConsistentHash{ns = 256, owners = (2)[fedora-7911: 133, fedora-23681: 123]}, pendingCH=ReplicatedConsistentHash{ns = 256, owners = (3)[fedora-7911: 90, fedora-23681: 84, fedora-14710: 82]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6, d6f750e4-03ef-442f-808f-9f13da616157]}
19:54:32,322 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 6
19:54:32,325 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t24) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 6
19:54:32,343 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 6
19:54:32,365 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=6, phase=READ_OLD_WRITE_ALL, rebalanceId=3, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-7911: 262+250, fedora-23681: 250+262]}, pendingCH=DefaultConsistentHash{ns=512, owners = (3)[fedora-7911: 180+181, fedora-23681: 162+162, fedora-14710: 170+169]}, unionCH=null, actualMembers=[fedora-7911, fedora-23681, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, 1b37b5c6-7d1a-443d-b7ed-0db51e36b1a6, d6f750e4-03ef-442f-808f-9f13da616157]}
19:54:32,366 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 6
19:54:32,369 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t3) [Context=testCache][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 6
19:54:32,377 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 6
19:54:32,383 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 6
19:54:32,384 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache ___protobuf_metadata, topology id = 6
19:54:32,386 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t15) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 7
19:54:32,389 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 7
19:54:32,395 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 7
19:54:32,397 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t11) [Context=___protobuf_metadata][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 8
19:54:32,401 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 8
19:54:32,407 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=___protobuf_metadata][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 8
19:54:32,411 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 6
19:54:32,412 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) ISPN000336: Finished cluster-wide rebalance for cache testCache, topology id = 6
19:54:32,414 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t20) [Context=testCache][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 7
19:54:32,417 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 7
19:54:32,421 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 7
19:54:32,424 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t1) [Context=testCache][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 8
19:54:32,426 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-23681]ISPN100003: Node fedora-23681 finished rebalance phase with topology id 8
19:54:32,432 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t2) [Context=testCache][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 8
19:54:32,578 INFO  [org.radargun.Slave] (sc-main) Starting stage AfterServiceStart
19:54:32,579 INFO  [org.radargun.Slave] (sc-main) Finished stage AfterServiceStart
19:54:32,581 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:54:32,727 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStart
19:54:32,742 INFO  [org.radargun.sysmonitor.AbstractMonitors] (sc-main) Gathering statistics every 1000 ms
19:54:32,742 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStart
19:54:32,753 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:54:32,786 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
19:54:40,710 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 10000 entries (~10000000 bytes)
19:54:46,890 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) This node loaded 20000 entries (~20000000 bytes)
19:54:51,104 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) This node loaded 30000 entries (~30000000 bytes)
19:54:52,017 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
19:54:52,017 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
19:54:52,060 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
19:54:52,105 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
19:54:52,105 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
19:54:52,126 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
19:54:52,129 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
19:54:52,130 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
19:54:52,150 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
19:54:52,162 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
19:54:52,164 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
19:54:52,164 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:54:52,277 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
19:54:52,283 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
19:54:52,284 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
19:54:52,284 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
19:54:52,285 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
19:54:52,285 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test warmup
19:54:52,311 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
19:55:52,322 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 1 mins 0 secs
19:55:52,324 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
19:55:52,335 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:55:52,349 INFO  [org.radargun.Slave] (sc-main) Starting stage Clear
19:55:52,353 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) Before executing clear, memory looks like this: 
Runtime free: 836,172 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,325 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 43,843 kb, init: 0 kb, committed: 45,256 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 12,503 kb, init: 2,496 kb, committed: 12,544 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 4,939 kb, init: 0 kb, committed: 5,504 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 364,544 kb, init: 73,728 kb, committed: 832,512 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 148,915 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 48,128 kb, init: 0 kb, committed: 48,128 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 5,071 kb, init: 2,496 kb, committed: 5,120 kb, max: 120,032 kb
19:55:52,507 INFO  [org.radargun.stages.cache.ClearStage] (sc-main) After executing clear, memory looks like this: 
Runtime free: 1,381,629 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,345 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 43,860 kb, init: 0 kb, committed: 45,256 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 12,606 kb, init: 2,496 kb, committed: 12,608 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 4,937 kb, init: 0 kb, committed: 5,504 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 0 kb, init: 73,728 kb, committed: 880,640 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 16,642 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 0 kb, init: 0 kb, committed: 0 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 5,082 kb, init: 2,496 kb, committed: 5,120 kb, max: 120,032 kb
19:55:52,507 INFO  [org.radargun.Slave] (sc-main) Finished stage Clear
19:55:52,508 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:55:52,551 INFO  [org.radargun.Slave] (sc-main) Starting stage Load
19:55:55,893 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) This node loaded 10000 entries (~10000000 bytes)
19:55:59,068 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) This node loaded 20000 entries (~20000000 bytes)
19:56:02,275 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) This node loaded 30000 entries (~30000000 bytes)
19:56:03,236 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-4) Finished loading entries
19:56:03,345 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-2) Finished loading entries
19:56:03,347 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-0) Finished loading entries
19:56:03,372 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-9) Finished loading entries
19:56:03,381 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-5) Finished loading entries
19:56:03,396 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-7) Finished loading entries
19:56:03,398 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-1) Finished loading entries
19:56:03,402 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-8) Finished loading entries
19:56:03,407 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-6) Finished loading entries
19:56:03,425 INFO  [org.radargun.stages.cache.test.LoadStage] (Loader-3) Finished loading entries
19:56:03,426 INFO  [org.radargun.Slave] (sc-main) Finished stage Load
19:56:03,426 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
19:56:03,807 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.stages.cache.test.BasicOperationsTestStage.init() overrides public void org.radargun.stages.test.TestStage.init() but both are declared with @Init annotation: calling only once
19:56:03,810 INFO  [org.radargun.Slave] (sc-main) Starting stage BasicOperationsTest
19:56:03,811 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using key generator org.radargun.stages.cache.generators.StringKeyGenerator {format=null }
19:56:03,812 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using value generator org.radargun.stages.cache.generators.ByteArrayValueGenerator { }
19:56:03,812 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Using cache selector Default { }
19:56:03,818 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Starting test stress-test
19:56:03,903 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Started 4 stressor threads.
20:06:03,907 INFO  [org.radargun.stages.cache.test.BasicOperationsTestStage] (sc-main) Finished test. Test duration is: 10 mins 0 secs
20:06:03,910 INFO  [org.radargun.Slave] (sc-main) Finished stage BasicOperationsTest
20:06:04,039 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
20:06:04,161 INFO  [org.radargun.Slave] (sc-main) Starting stage MonitorStop
20:06:04,163 INFO  [org.radargun.Slave] (sc-main) Finished stage MonitorStop
20:06:04,164 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
20:06:04,169 INFO  [org.radargun.Slave] (sc-main) Starting stage ScenarioDestroy
20:06:04,171 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Scenario finished, destroying...
20:06:04,172 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Memory before cleanup: 
Runtime free: 436,989 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,328 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 44,618 kb, init: 0 kb, committed: 46,024 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 14,501 kb, init: 2,496 kb, committed: 14,528 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 4,978 kb, init: 0 kb, committed: 5,504 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 580,608 kb, init: 73,728 kb, committed: 812,032 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 318,722 kb, init: 1,325,056 kb, committed: 526,336 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 60,416 kb, init: 0 kb, committed: 60,416 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 6,109 kb, init: 2,496 kb, committed: 6,144 kb, max: 120,032 kb
20:06:04,173 INFO  [org.radargun.stages.lifecycle.LifecycleHelper] (sc-main) Stopping service.
20:06:04,185 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t7) ISPN000310: Starting cluster-wide rebalance for cache testCache, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-7911: 273+88, fedora-14710: 239+100]}, pendingCH=DefaultConsistentHash{ns=512, owners = (2)[fedora-7911: 267+245, fedora-14710: 245+267]}, unionCH=null, actualMembers=[fedora-7911, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, d6f750e4-03ef-442f-808f-9f13da616157]}
20:06:04,186 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t7) [Context=testCache][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 11
20:06:04,208 WARN  [org.infinispan.CLUSTER] (StopThread) [Context=testCache]ISPN000312: Lost data because of graceful leaver fedora-7911
20:06:04,245 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t7) ISPN000310: Starting cluster-wide rebalance for cache ___counters, topology CacheTopology{id=11, phase=READ_OLD_WRITE_ALL, rebalanceId=4, currentCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-7911: 136+52, fedora-14710: 120+40]}, pendingCH=DefaultConsistentHash{ns=256, owners = (2)[fedora-7911: 134+122, fedora-14710: 122+134]}, unionCH=null, actualMembers=[fedora-7911, fedora-14710], persistentUUIDs=[0f136f55-3cde-466a-b2d9-8fcf5910813a, d6f750e4-03ef-442f-808f-9f13da616157]}
20:06:04,247 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t7) [Context=___counters][Scope=fedora-7911]ISPN100002: Started rebalance with topology id 11
20:06:04,257 INFO  [org.infinispan.CLUSTER] (remote-thread--p2-t7) [Context=testCache][Scope=fedora-14710]ISPN100003: Node fedora-14710 finished rebalance phase with topology id 12
20:06:04,268 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t1) ISPN000210: Failed to request state of cache testCache from node fedora-14710, segments {0 12 18-19 30-34 38 44 58-59 69 88-93 109-112 124-128 132-133 136 139-146 152-154 159 171-174 178 185-186 190 193-195 204 208-209 213-215 223-227 244-247 254 260 271-276 281-282 285 294-296 306 311-312 332 335-337 342 347-350 355-358 372 376-381 385-387 390 398 406-407 422-424 431 437-439 445-446 457-459 463-469 472-479 488-489 499 505-511}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-14710 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
20:06:04,271 WARN  [org.infinispan.CLUSTER] (remote-thread--p2-t29) [Context=___counters]ISPN000312: Lost data because of graceful leaver fedora-14710
20:06:04,272 WARN  [org.infinispan.statetransfer.InboundTransferTask] (stateTransferExecutor-thread--p6-t2) ISPN000210: Failed to request state of cache ___counters from node fedora-14710, segments {9 14-16 19-22 28-29 40 44-45 59 68-72 76-77 97 112 122-123 127 136-141 146-147 153-156 168 173-174 177-178 184-186 208 211-212 227-235 241-243 248-249 251-255}
org.infinispan.remoting.transport.jgroups.SuspectException: ISPN000400: Node fedora-14710 was suspected
	at org.infinispan.remoting.transport.ResponseCollectors.remoteNodeSuspected(ResponseCollectors.java:33) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:31) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleResponseCollector.targetNotFound(SingleResponseCollector.java:17) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.ValidSingleResponseCollector.addResponse(ValidSingleResponseCollector.java:23) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.receiveResponse(SingleTargetRequest.java:51) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.SingleTargetRequest.onResponse(SingleTargetRequest.java:35) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.impl.RequestRepository.addResponse(RequestRepository.java:53) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processResponse(JGroupsTransport.java:1302) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.processMessage(JGroupsTransport.java:1205) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport.access$200(JGroupsTransport.java:123) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.remoting.transport.jgroups.JGroupsTransport$ChannelCallbacks.receive(JGroupsTransport.java:1340) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.jgroups.JChannel.up(JChannel.java:819) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.stack.ProtocolStack.up(ProtocolStack.java:893) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FRAG3.up(FRAG3.java:171) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FlowControl.up(FlowControl.java:343) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.GMS.up(GMS.java:864) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.STABLE.up(STABLE.java:240) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.deliverMessage(UNICAST3.java:1002) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.handleDataReceived(UNICAST3.java:728) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.UNICAST3.up(UNICAST3.java:383) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.pbcast.NAKACK2.up(NAKACK2.java:600) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.VERIFY_SUSPECT.up(VERIFY_SUSPECT.java:119) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_ALL.up(FD_ALL.java:199) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.FD_SOCK.up(FD_SOCK.java:252) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.MERGE3.up(MERGE3.java:276) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.Discovery.up(Discovery.java:267) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.protocols.TP.passMessageUp(TP.java:1248) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at org.jgroups.util.SubmitToThreadPool$SingleMessageHandler.run(SubmitToThreadPool.java:87) ~[jgroups-4.0.10.Final.jar:4.0.10.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
20:06:04,284 ERROR [org.infinispan.statetransfer.StateConsumerImpl] (transport-thread--p4-t6) ISPN000208: No live owners found for segments {9 14-16 19-22 28-29 40 44-45 59 68-72 76-77 97 112 122-123 127 136-141 146-147 153-156 168 173-174 177-178 184-186 208 211-212 227-235 241-243 248-249 251-255} of cache ___counters. Excluded owners: []
20:06:04,286 INFO  [org.infinispan.CLUSTER] (transport-thread--p4-t6) [Context=___counters][Scope=fedora-7911]ISPN100003: Node fedora-7911 finished rebalance phase with topology id 12
20:06:04,288 WARN  [org.infinispan.topology.CacheTopologyControlCommand] (transport-thread--p4-t6) ISPN000071: Caught exception when handling command CacheTopologyControlCommand{cache=___counters, type=REBALANCE_PHASE_CONFIRM, sender=fedora-7911, joinInfo=null, topologyId=12, rebalanceId=4, currentCH=null, pendingCH=null, availabilityMode=null, phase=null, actualMembers=null, throwable=null, viewId=2}
org.infinispan.commons.CacheException: Received invalid rebalance confirmation from fedora-7911 for cache ___counters, we don't have a rebalance in progress
	at org.infinispan.topology.ClusterCacheStatus.confirmRebalancePhase(ClusterCacheStatus.java:333) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.ClusterTopologyManagerImpl.handleRebalancePhaseConfirm(ClusterTopologyManagerImpl.java:258) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.doPerform(CacheTopologyControlCommand.java:183) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.CacheTopologyControlCommand.invokeAsync(CacheTopologyControlCommand.java:160) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.commands.ReplicableCommand.invoke(ReplicableCommand.java:44) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at org.infinispan.topology.LocalTopologyManagerImpl.lambda$executeOnCoordinatorAsync$4(LocalTopologyManagerImpl.java:717) ~[infinispan-core-9.2.0.Final.jar:9.2.0.Final]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
20:06:04,289 INFO  [org.infinispan.CLUSTER] (jgroups-54,fedora-7911) ISPN000094: Received new cluster view for channel results: [fedora-7911|3] (2) [fedora-7911, fedora-14710]
20:06:04,292 INFO  [org.infinispan.CLUSTER] (jgroups-54,fedora-7911) ISPN100001: Node fedora-23681 left the cluster
20:06:04,303 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (StopThread) ISPN000080: Disconnecting JGroups channel results
20:06:04,358 INFO  [org.radargun.service.Infinispan90Lifecycle] (StopThread) Stopped, previous view is [fedora-7911, fedora-23681, fedora-14710]
20:06:04,359 INFO  [org.radargun.stages.ScenarioDestroyStage] (sc-main) Service successfully stopped.
20:06:04,359 INFO  [org.radargun.Slave] (sc-main) Finished stage ScenarioDestroy
20:06:04,359 INFO  [org.radargun.RemoteMasterConnection] (sc-main) Message successfully sent to the master
20:06:04,373 WARN  [org.radargun.config.InitHelper] (sc-main) Method public void org.radargun.service.Infinispan80EmbeddedService.destroy() overrides public void org.radargun.service.Infinispan60EmbeddedService.destroy() but both are declared with @Destroy annotation: calling only once
20:06:04,376 INFO  [org.radargun.Slave] (main) Starting stage ScenarioCleanup
20:06:04,382 WARN  [org.radargun.stages.ScenarioCleanupStage] (main) Unfinished thread ForkJoinPool.commonPool-worker-3 (id=33, state=WAITING)
	at java.base@11.0.14-internal/jdk.internal.misc.Unsafe.park(Native Method)
	at java.base@11.0.14-internal/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
	at java.base@11.0.14-internal/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628)
	at java.base@11.0.14-internal/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
20:06:04,383 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Interrupting thread ForkJoinPool.commonPool-worker-3 (id=33, state=WAITING)
20:06:09,387 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Stopping thread ForkJoinPool.commonPool-worker-3 (id=33, state=WAITING)
20:06:09,397 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Is thread ForkJoinPool.commonPool-worker-3 (id=33, state=TERMINATED)) alive? false
20:06:09,584 INFO  [org.radargun.stages.ScenarioCleanupStage] (main) Memory after cleanup: 
Runtime free: 1,380,569 kb
Runtime max:1,398,784 kb
Runtime total:1,398,784 kb
MX CodeHeap 'non-nmethods'(Non-heap memory): used: 1,329 kb, init: 2,496 kb, committed: 2,496 kb, max: 5,696 kb
MX Metaspace(Non-heap memory): used: 44,772 kb, init: 0 kb, committed: 46,280 kb, max: 0 kb
MX CodeHeap 'profiled nmethods'(Non-heap memory): used: 14,646 kb, init: 2,496 kb, committed: 14,656 kb, max: 120,032 kb
MX Compressed Class Space(Non-heap memory): used: 4,999 kb, init: 0 kb, committed: 5,504 kb, max: 1,048,576 kb
MX G1 Eden Space(Heap memory): used: 0 kb, init: 73,728 kb, committed: 880,640 kb, max: 0 kb
MX G1 Old Gen(Heap memory): used: 17,702 kb, init: 1,325,056 kb, committed: 518,144 kb, max: 1,398,784 kb
MX G1 Survivor Space(Heap memory): used: 0 kb, init: 0 kb, committed: 0 kb, max: 0 kb
MX CodeHeap 'non-profiled nmethods'(Non-heap memory): used: 6,174 kb, init: 2,496 kb, committed: 6,208 kb, max: 120,032 kb
20:06:09,586 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
20:06:09,636 INFO  [org.radargun.RemoteMasterConnection] (main) Message successfully sent to the master
20:06:13,412 INFO  [org.radargun.Slave] (main) Master shutdown!
20:06:13,418 INFO  [org.radargun.ShutDownHook] (Thread-0) Slave process is being shutdown
